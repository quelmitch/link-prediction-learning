{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1867763",
   "metadata": {},
   "source": [
    "# Gowalla - Modelling\n",
    "After looking at the data in the EDA, it's time to build our prediction model. The main goal is to predict if two users will be friends based on their behavior and their place in the network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54098aa9",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c9d910",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "from haversine import haversine_vector\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9e88123",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "\n",
    "pd.set_option('display.float_format', '{:.10f}'.format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8672b6ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "edges_path = 'data/Gowalla_edges.txt'\n",
    "\n",
    "# Data loading\n",
    "G = nx.read_edgelist(edges_path, nodetype=int)\n",
    "checkins_df = pd.read_parquet('data/EDA_output/checkins.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49632a22",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef635e1c",
   "metadata": {},
   "source": [
    "Initially, a simple random 80/20 split of the edges was considered for creating the training and test sets. However, this approach was identified as methodologically flawed for a temporal dataset. A random split could create a scenario where the model is trained on check-in features from 2010 to predict a link that was held out for the test set, even if that link represents a friendship from 2009. This constitutes a form of data leakage, where information from the future is used to predict the past, leading to an overly optimistic and unrealistic evaluation of the model's performance.\"\n",
    "\n",
    "To avoid this temporal leakage and address the limitations of the data, we reframe the prediction task. Given the absence of friendship formation timestamps that makes true temporal prediction impossible, we *shift from predicting link formation to predicting link existence within the final network snapshot*.\n",
    "\n",
    "> Can we use a user's early activity to effectively distinguish between user pairs who are friends and those who are not in the final, complete social network?\n",
    "\n",
    "In this new objective we accept **structural leakage** by using the final graph for embeddings, but we explicitly define the task around this by predicting the *final state*, not the *formation*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5897528",
   "metadata": {},
   "source": [
    "### Feature Dataset Train/Test Split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7fcbad",
   "metadata": {},
   "source": [
    "We proceed to make the 80/20 split of the check-ins. We have two alternative:\n",
    "- Split by volume of check-ins (e.g. train set will have 80% of check-ins)\n",
    "- Split by time passed (e.g train set will have check-ins that appears in the first 80% of time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f585fdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check-ins cutoff at:  2010-06-20 00:33:12.400000+00:00\n",
      "Train set size: 2939173 - Corresponding percentage of volume: 45.6\n",
      "Test set size:  3503690 - Corresponding percentage of volume: 54.4\n"
     ]
    }
   ],
   "source": [
    "# Split the check-ins dataframe by time\n",
    "min_ts = checkins_df['check-in_datetime'].min()\n",
    "max_ts = checkins_df['check-in_datetime'].max()\n",
    "\n",
    "time_diff = max_ts - min_ts\n",
    "\n",
    "time_cutoff = min_ts + pd.to_timedelta(0.8 * time_diff)\n",
    "\n",
    "t_train_df = checkins_df[checkins_df['check-in_datetime'] < time_cutoff]\n",
    "t_test_df = checkins_df[checkins_df['check-in_datetime'] >= time_cutoff]\n",
    "\n",
    "print(\"Check-ins cutoff at: \", time_cutoff)\n",
    "print(f\"Train set size: {len(t_train_df)} - Corresponding percentage of volume: {len(t_train_df)/len(checkins_df)*100:.1f}\")\n",
    "print(f\"Test set size:  {len(t_test_df)} - Corresponding percentage of volume: {len(t_test_df)/len(checkins_df)*100:.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0f1e30e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check-ins cutoff at:  2010-09-14 14:24:41+00:00\n",
      "Train set size: 5154291 - Corresponding percentage of time: 93.8\n",
      "Test set size:  1288572 - Corresponding percentage of time: 6.2\n"
     ]
    }
   ],
   "source": [
    "# Split the check-ins dataframe by volume\n",
    "volume_cutoff = math.ceil(len(checkins_df) * 0.80)\n",
    "\n",
    "checkins_df = checkins_df.sort_values(by=['check-in_datetime'])\n",
    "\n",
    "v_train_df = checkins_df.iloc[:volume_cutoff]\n",
    "v_test_df = checkins_df.iloc[volume_cutoff:]\n",
    "\n",
    "volume_cutoff_ts = checkins_df.iloc[volume_cutoff]['check-in_datetime']\n",
    "\n",
    "print(\"Check-ins cutoff at: \", volume_cutoff_ts)\n",
    "print(f\"Train set size: {len(v_train_df)} - Corresponding percentage of time: {((volume_cutoff_ts - min_ts) / time_diff) * 100:.1f}\")\n",
    "print(f\"Test set size:  {len(v_test_df)} - Corresponding percentage of time: {((max_ts - volume_cutoff_ts) / time_diff) * 100:.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ddd4050",
   "metadata": {},
   "source": [
    "Either way we obtain an overall imbalanced split, and either options have pros and cons:\n",
    "- Option \"Time\": This is the most honest representation of a real-world temporal prediction task, but the features built from this sparser data might be weak.\n",
    "- Option \"Volume\": The features built will be much stronger and the model will surely perform better, but the separation between past and future is tiny.\n",
    "\n",
    "As the goal of the notebook is learning, we proceed with the time-based split."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e13a1d9",
   "metadata": {},
   "source": [
    "### Negative Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c8ece56",
   "metadata": {},
   "source": [
    "To create a balanced dataset we take an equal number of positive and negative samples. To make the training set challenging, we use a mixed strategy:\n",
    "- 70% of negative samples are \"hard\" negatives generated with random walks with a path distance of 2.\n",
    "- 20% are \"medium\" negatives generated with random walks with a path distance of 4, representing users who are further apart but still connected.\n",
    "- 10% are \"easy\" random negatives, to ensure the model learns the global structure of the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c3d860a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Negative Sampling Generation Functions\n",
    "def _random_walk_hard_negative(graph, count, walk_distance=2, max_attempts=100):\n",
    "    \"\"\"Generates a batch of hard negative samples.\"\"\"\n",
    "    if walk_distance < 1:\n",
    "        raise ValueError(\"walk_distance must be >= 1\")\n",
    "\n",
    "    edges = list(graph.edges())\n",
    "    if not edges:\n",
    "        return []\n",
    "\n",
    "    generated_samples = []\n",
    "    for _ in range(count):\n",
    "        for _ in range(max_attempts):\n",
    "            u, current_node = random.choice(edges)\n",
    "\n",
    "            # Make hops\n",
    "            for _ in range(walk_distance - 1):\n",
    "                neighbors = list(graph.neighbors(current_node))\n",
    "                if not neighbors:\n",
    "                    current_node = None\n",
    "                    break\n",
    "                current_node = random.choice(neighbors)\n",
    "\n",
    "            if current_node is None:\n",
    "                continue\n",
    "\n",
    "            v = current_node\n",
    "\n",
    "            # Validate edge and add to the batch\n",
    "            if v != u and not graph.has_edge(u, v):\n",
    "                generated_samples.append((u, v))\n",
    "                break\n",
    "\n",
    "    return generated_samples\n",
    "\n",
    "def _random_negative_sampling(graph, count, max_attempts=100):\n",
    "    \"\"\"Generates a batch of random negative samples.\"\"\"\n",
    "    nodes = list(graph.nodes())\n",
    "    if len(nodes) < 2:\n",
    "        return []\n",
    "\n",
    "    generated_samples = []\n",
    "    for _ in range(count):\n",
    "        for _ in range(max_attempts):\n",
    "            u, v = random.sample(nodes, 2)\n",
    "            if not graph.has_edge(u, v):\n",
    "                generated_samples.append((u, v))\n",
    "                break\n",
    "\n",
    "    return generated_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b4c46f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main Negative Sampling Function to combine different methods\n",
    "def generate_negative_samples_set(\n",
    "    graph,\n",
    "    sampling_functions,\n",
    "    sampling_weights,\n",
    "    ratio=1.0,\n",
    "    seed=None\n",
    "):\n",
    "    if seed is not None:\n",
    "        random.seed(seed)\n",
    "        np.random.seed(seed)\n",
    "\n",
    "    if len(sampling_functions) != len(sampling_weights):\n",
    "        raise ValueError(\"sampling_functions and sampling_weights must be of the same length\")\n",
    "\n",
    "    total_neg_samples = int(len(graph.edges) * ratio)\n",
    "\n",
    "    weights = np.array(sampling_weights, dtype=float)\n",
    "    weights /= weights.sum()\n",
    "    samples_for_each_function = np.random.multinomial(total_neg_samples, weights)\n",
    "\n",
    "    negative_samples = set()\n",
    "\n",
    "    for func, count in zip(sampling_functions, samples_for_each_function):\n",
    "        if count == 0:\n",
    "            continue\n",
    "\n",
    "        new_samples = func(graph, count)\n",
    "        negative_samples.update(new_samples)\n",
    "\n",
    "    return negative_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ed9d1a56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 973408 (102.43%) negative samples.\n"
     ]
    }
   ],
   "source": [
    "# Generate Negative Samples\n",
    "positive_samples = list(G.edges)\n",
    "\n",
    "sampling_functions = [\n",
    "    lambda g, c: _random_negative_sampling(g, c),\n",
    "    lambda g, c: _random_walk_hard_negative(g, c, walk_distance=4),\n",
    "    lambda g, c: _random_walk_hard_negative(g, c, walk_distance=2)\n",
    "]\n",
    "\n",
    "sampling_weights = [0.1, 0.2, 0.7]\n",
    "\n",
    "negative_samples = generate_negative_samples_set(\n",
    "    graph=G,\n",
    "    sampling_functions=sampling_functions,\n",
    "    sampling_weights=sampling_weights,\n",
    "    ratio=1.1, # Allow to compensate duplicates\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "print(f\"Generated {len(negative_samples)} ({len(negative_samples)/len(positive_samples)*100:.2f}%) negative samples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "04954f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Truncate the exceeding part\n",
    "negative_samples = random.sample(list(negative_samples), len(positive_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6c2c510a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine and create labels\n",
    "combined_samples = positive_samples + negative_samples\n",
    "samples_labels = [1] * len(positive_samples) + [0] * len(negative_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5fda52e",
   "metadata": {},
   "source": [
    "### Node Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dbf3f9d",
   "metadata": {},
   "source": [
    "The goal of **Node Embedding** is to learn a *dense vector representation* for each user that captures their neighborhood and role in the network. This approach allows the model to automatically learn features from the graph topology, serving as a powerful alternative to manual feature engineering with standard measures (e.g., Common Neighbors).\n",
    "\n",
    "We chose the **Node2Vec** algorithm for this task. An initial test with a standard Python implementation of Node2Vec proved to be extremely slow. The first phase of the algorithm, *\"computing transition probabilities\"* was single-threaded and was projected to take several hours. A Stack Overflow discussion<sup>[[1][1]]</sup> highlighted this common bottleneck and pointed towards more performant, specialized libraries.\n",
    "\n",
    "[1]: https://stackoverflow.com/questions/60276191/is-there-any-way-to-make-node2vec-faster\n",
    "\n",
    "Based on this research, we switched to **GRAPE**<sup>[[2][2]]</sup>, a fast graph processing and embedding library. GRAPE is written in *Rust* and *Python*, developed primarily at the University of Milan, and is designed for speed and scalability on massive graphs. It parallelizes the entire Node2Vec process, including the pre-computation step, which dramatically reduces the training time.  \n",
    "While the library's documentation was found to be in a raw state, we were able to successfully implement it for our purposes.\n",
    "\n",
    "[2]: https://github.com/AnacletoLAB/grape\n",
    "\n",
    "To make our workflow efficient and reproducible, we performed this computationally expensive step in a separate script (`train_embeddings.py`) and saved the resulting embeddings to a file. The model was configured with a neutral baseline set of hyperparameters:\n",
    "- `embedding_size` (dimensions): 128\n",
    "- `walk_length` (length of each walk): 80\n",
    "- `iterations` (number of walks per node): 10\n",
    "- `return_weight` ($p = 1 / \\text{return\\_weight}$): 1.0\n",
    "- `explore_weight` ($q = 1 / \\text{explore\\_weight}$): 1.0\n",
    "\n",
    "In the Node2Vec framework `return_weight` and `explore_weight` set to `1.0` corresponds to the parameters `p` and `q`, which removes any search bias and makes the algorithm equivalent to the **DeepWalk** algorithm.\n",
    "\n",
    "The training process, which ran over *30 epochs*, required *35 minutes* to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "22834935",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Embeddings\n",
    "embedding_path = \"embeddings/gowalla_node2vec_embeddings.parquet\"\n",
    "embedding_df = pd.read_parquet(embedding_path)\n",
    "embedding_df.index = embedding_df.index.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3d817e3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>118</th>\n",
       "      <th>119</th>\n",
       "      <th>120</th>\n",
       "      <th>121</th>\n",
       "      <th>122</th>\n",
       "      <th>123</th>\n",
       "      <th>124</th>\n",
       "      <th>125</th>\n",
       "      <th>126</th>\n",
       "      <th>127</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-70.2993240356</td>\n",
       "      <td>104.7642517090</td>\n",
       "      <td>214.4183197021</td>\n",
       "      <td>32.5812606812</td>\n",
       "      <td>45.5297737122</td>\n",
       "      <td>-224.9113006592</td>\n",
       "      <td>-34.7072448730</td>\n",
       "      <td>-298.1494750977</td>\n",
       "      <td>129.5424499512</td>\n",
       "      <td>67.4690933228</td>\n",
       "      <td>...</td>\n",
       "      <td>-105.4124908447</td>\n",
       "      <td>-51.9473838806</td>\n",
       "      <td>-102.9604339600</td>\n",
       "      <td>-141.7391204834</td>\n",
       "      <td>204.9244232178</td>\n",
       "      <td>-261.5464477539</td>\n",
       "      <td>-77.5561447144</td>\n",
       "      <td>19.7141284943</td>\n",
       "      <td>23.8973484039</td>\n",
       "      <td>195.5611419678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-260.2064819336</td>\n",
       "      <td>-104.6907653809</td>\n",
       "      <td>31.3309593201</td>\n",
       "      <td>119.9266281128</td>\n",
       "      <td>-124.3474426270</td>\n",
       "      <td>135.4678497314</td>\n",
       "      <td>0.4026212692</td>\n",
       "      <td>-39.9399147034</td>\n",
       "      <td>61.0377693176</td>\n",
       "      <td>151.5090789795</td>\n",
       "      <td>...</td>\n",
       "      <td>325.2360534668</td>\n",
       "      <td>-9.6202507019</td>\n",
       "      <td>19.4639568329</td>\n",
       "      <td>-81.2915039062</td>\n",
       "      <td>26.7115669250</td>\n",
       "      <td>-47.5153923035</td>\n",
       "      <td>-77.6566009521</td>\n",
       "      <td>-122.7394409180</td>\n",
       "      <td>87.3364486694</td>\n",
       "      <td>-115.6673278809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-91.6686706543</td>\n",
       "      <td>-46.4467735291</td>\n",
       "      <td>86.5811386108</td>\n",
       "      <td>-220.1943511963</td>\n",
       "      <td>12.7583608627</td>\n",
       "      <td>8.1809272766</td>\n",
       "      <td>-22.7226924896</td>\n",
       "      <td>-86.8817749023</td>\n",
       "      <td>-82.5780715942</td>\n",
       "      <td>-254.7404327393</td>\n",
       "      <td>...</td>\n",
       "      <td>158.5266723633</td>\n",
       "      <td>258.4648742676</td>\n",
       "      <td>-213.0023193359</td>\n",
       "      <td>-65.7916030884</td>\n",
       "      <td>264.3765258789</td>\n",
       "      <td>80.9072494507</td>\n",
       "      <td>-95.8889389038</td>\n",
       "      <td>-58.6533508301</td>\n",
       "      <td>-153.2696228027</td>\n",
       "      <td>-241.1257171631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>73.5213851929</td>\n",
       "      <td>-146.0536651611</td>\n",
       "      <td>-68.6905746460</td>\n",
       "      <td>224.7186889648</td>\n",
       "      <td>-89.2506484985</td>\n",
       "      <td>-123.0056457520</td>\n",
       "      <td>-43.8369445801</td>\n",
       "      <td>-157.7464294434</td>\n",
       "      <td>-73.6201477051</td>\n",
       "      <td>-41.8115425110</td>\n",
       "      <td>...</td>\n",
       "      <td>238.8144989014</td>\n",
       "      <td>-32.8614349365</td>\n",
       "      <td>121.9555130005</td>\n",
       "      <td>230.7531433105</td>\n",
       "      <td>91.0168304443</td>\n",
       "      <td>-223.9833679199</td>\n",
       "      <td>-84.6682815552</td>\n",
       "      <td>125.3443145752</td>\n",
       "      <td>-201.8149261475</td>\n",
       "      <td>-279.3994750977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.6959481239</td>\n",
       "      <td>1.2965914011</td>\n",
       "      <td>-2.1391603947</td>\n",
       "      <td>0.3263847530</td>\n",
       "      <td>1.1603585482</td>\n",
       "      <td>0.7213216424</td>\n",
       "      <td>0.4505686462</td>\n",
       "      <td>-1.4355301857</td>\n",
       "      <td>1.0976371765</td>\n",
       "      <td>0.3337603509</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.6340924501</td>\n",
       "      <td>-0.7793192863</td>\n",
       "      <td>2.8571474552</td>\n",
       "      <td>0.3815542758</td>\n",
       "      <td>1.8547649384</td>\n",
       "      <td>-1.0311043262</td>\n",
       "      <td>2.7614219189</td>\n",
       "      <td>0.5874806643</td>\n",
       "      <td>0.7654225230</td>\n",
       "      <td>-1.9500918388</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 128 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0               1              2               3    \\\n",
       "0  -70.2993240356  104.7642517090 214.4183197021   32.5812606812   \n",
       "1 -260.2064819336 -104.6907653809  31.3309593201  119.9266281128   \n",
       "2  -91.6686706543  -46.4467735291  86.5811386108 -220.1943511963   \n",
       "3   73.5213851929 -146.0536651611 -68.6905746460  224.7186889648   \n",
       "4   -0.6959481239    1.2965914011  -2.1391603947    0.3263847530   \n",
       "\n",
       "              4               5              6               7    \\\n",
       "0   45.5297737122 -224.9113006592 -34.7072448730 -298.1494750977   \n",
       "1 -124.3474426270  135.4678497314   0.4026212692  -39.9399147034   \n",
       "2   12.7583608627    8.1809272766 -22.7226924896  -86.8817749023   \n",
       "3  -89.2506484985 -123.0056457520 -43.8369445801 -157.7464294434   \n",
       "4    1.1603585482    0.7213216424   0.4505686462   -1.4355301857   \n",
       "\n",
       "             8               9    ...             118            119  \\\n",
       "0 129.5424499512   67.4690933228  ... -105.4124908447 -51.9473838806   \n",
       "1  61.0377693176  151.5090789795  ...  325.2360534668  -9.6202507019   \n",
       "2 -82.5780715942 -254.7404327393  ...  158.5266723633 258.4648742676   \n",
       "3 -73.6201477051  -41.8115425110  ...  238.8144989014 -32.8614349365   \n",
       "4   1.0976371765    0.3337603509  ...   -1.6340924501  -0.7793192863   \n",
       "\n",
       "              120             121            122             123  \\\n",
       "0 -102.9604339600 -141.7391204834 204.9244232178 -261.5464477539   \n",
       "1   19.4639568329  -81.2915039062  26.7115669250  -47.5153923035   \n",
       "2 -213.0023193359  -65.7916030884 264.3765258789   80.9072494507   \n",
       "3  121.9555130005  230.7531433105  91.0168304443 -223.9833679199   \n",
       "4    2.8571474552    0.3815542758   1.8547649384   -1.0311043262   \n",
       "\n",
       "             124             125             126             127  \n",
       "0 -77.5561447144   19.7141284943   23.8973484039  195.5611419678  \n",
       "1 -77.6566009521 -122.7394409180   87.3364486694 -115.6673278809  \n",
       "2 -95.8889389038  -58.6533508301 -153.2696228027 -241.1257171631  \n",
       "3 -84.6682815552  125.3443145752 -201.8149261475 -279.3994750977  \n",
       "4   2.7614219189    0.5874806643    0.7654225230   -1.9500918388  \n",
       "\n",
       "[5 rows x 128 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show result\n",
    "embedding_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c0b8cf35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame with 196591 rows and 128 columns\n"
     ]
    }
   ],
   "source": [
    "print(f'DataFrame with {embedding_df.shape[0]} rows and {embedding_df.shape[1]} columns')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "941ffcea",
   "metadata": {},
   "source": [
    "The next step is to create a single feature vector for each *pair* of users in our sample set. This vector needs to describe the relationship between the two users. The **Hadamard Product** is a simple yet effective method that should capture the interaction and agreement between two users embedding along each dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f3a84828",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Efficient vectorized Hadamard Product\n",
    "samples_df = pd.DataFrame(combined_samples, columns=['source', 'destination'])\n",
    "source_vectors = embedding_df.loc[(samples_df['source'])].values\n",
    "destination_vectors = embedding_df.loc[samples_df['destination']].values\n",
    "hadamard_product = source_vectors * destination_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1a12fe4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Pair Features dataframe with multi-index\n",
    "pair_features_df = pd.DataFrame(\n",
    "    hadamard_product,\n",
    "    index=pd.MultiIndex.from_frame(samples_df),\n",
    "    columns=[f'embed_hadamard_{i}' for i in range(hadamard_product.shape[1])]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d587d1a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>embed_hadamard_0</th>\n",
       "      <th>embed_hadamard_1</th>\n",
       "      <th>embed_hadamard_2</th>\n",
       "      <th>embed_hadamard_3</th>\n",
       "      <th>embed_hadamard_4</th>\n",
       "      <th>embed_hadamard_5</th>\n",
       "      <th>embed_hadamard_6</th>\n",
       "      <th>embed_hadamard_7</th>\n",
       "      <th>embed_hadamard_8</th>\n",
       "      <th>embed_hadamard_9</th>\n",
       "      <th>...</th>\n",
       "      <th>embed_hadamard_118</th>\n",
       "      <th>embed_hadamard_119</th>\n",
       "      <th>embed_hadamard_120</th>\n",
       "      <th>embed_hadamard_121</th>\n",
       "      <th>embed_hadamard_122</th>\n",
       "      <th>embed_hadamard_123</th>\n",
       "      <th>embed_hadamard_124</th>\n",
       "      <th>embed_hadamard_125</th>\n",
       "      <th>embed_hadamard_126</th>\n",
       "      <th>embed_hadamard_127</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>source</th>\n",
       "      <th>destination</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">0</th>\n",
       "      <th>1</th>\n",
       "      <td>18292.3398437500</td>\n",
       "      <td>-10967.8496093750</td>\n",
       "      <td>6717.9316406250</td>\n",
       "      <td>3907.3608398438</td>\n",
       "      <td>-5661.5107421875</td>\n",
       "      <td>-30468.2500000000</td>\n",
       "      <td>-13.9738750458</td>\n",
       "      <td>11908.0644531250</td>\n",
       "      <td>7906.9819335938</td>\n",
       "      <td>10222.1806640625</td>\n",
       "      <td>...</td>\n",
       "      <td>-34283.9414062500</td>\n",
       "      <td>499.7468566895</td>\n",
       "      <td>-2004.0174560547</td>\n",
       "      <td>11522.1865234375</td>\n",
       "      <td>5473.8525390625</td>\n",
       "      <td>12427.4824218750</td>\n",
       "      <td>6022.7465820312</td>\n",
       "      <td>-2419.7011718750</td>\n",
       "      <td>2087.1096191406</td>\n",
       "      <td>-22620.0351562500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6444.2456054688</td>\n",
       "      <td>-4865.9614257812</td>\n",
       "      <td>18564.5820312500</td>\n",
       "      <td>-7174.2094726562</td>\n",
       "      <td>580.8852539062</td>\n",
       "      <td>-1839.9830322266</td>\n",
       "      <td>788.6420288086</td>\n",
       "      <td>25903.7558593750</td>\n",
       "      <td>-10697.3652343750</td>\n",
       "      <td>-17187.1054687500</td>\n",
       "      <td>...</td>\n",
       "      <td>-16710.6914062500</td>\n",
       "      <td>-13426.5742187500</td>\n",
       "      <td>21930.8105468750</td>\n",
       "      <td>9325.2441406250</td>\n",
       "      <td>54177.2070312500</td>\n",
       "      <td>-21161.0039062500</td>\n",
       "      <td>7436.7763671875</td>\n",
       "      <td>-1156.2996826172</td>\n",
       "      <td>-3662.7375488281</td>\n",
       "      <td>-47154.8203125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-5168.5039062500</td>\n",
       "      <td>-15301.2031250000</td>\n",
       "      <td>-14728.5175781250</td>\n",
       "      <td>7321.6181640625</td>\n",
       "      <td>-4063.5617675781</td>\n",
       "      <td>27665.3593750000</td>\n",
       "      <td>1521.4595947266</td>\n",
       "      <td>47032.0156250000</td>\n",
       "      <td>-9536.9345703125</td>\n",
       "      <td>-2820.9868164062</td>\n",
       "      <td>...</td>\n",
       "      <td>-25174.0312500000</td>\n",
       "      <td>1707.0655517578</td>\n",
       "      <td>-12556.5927734375</td>\n",
       "      <td>-32706.7480468750</td>\n",
       "      <td>18651.5722656250</td>\n",
       "      <td>58582.0546875000</td>\n",
       "      <td>6566.5454101562</td>\n",
       "      <td>2471.0539550781</td>\n",
       "      <td>-4822.8417968750</td>\n",
       "      <td>-54639.6796875000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>48.9246826172</td>\n",
       "      <td>135.8364257812</td>\n",
       "      <td>-458.6751708984</td>\n",
       "      <td>10.6340265274</td>\n",
       "      <td>52.8308639526</td>\n",
       "      <td>-162.2333831787</td>\n",
       "      <td>-15.6379966736</td>\n",
       "      <td>428.0025634766</td>\n",
       "      <td>142.1906127930</td>\n",
       "      <td>22.5185089111</td>\n",
       "      <td>...</td>\n",
       "      <td>172.2537536621</td>\n",
       "      <td>40.4835968018</td>\n",
       "      <td>-294.1731567383</td>\n",
       "      <td>-54.0811691284</td>\n",
       "      <td>380.0866394043</td>\n",
       "      <td>269.6816711426</td>\n",
       "      <td>-214.1652374268</td>\n",
       "      <td>11.5816688538</td>\n",
       "      <td>18.2915687561</td>\n",
       "      <td>-381.3621826172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-5373.4765625000</td>\n",
       "      <td>21647.4687500000</td>\n",
       "      <td>-9167.5957031250</td>\n",
       "      <td>-2164.2231445312</td>\n",
       "      <td>-8469.9921875000</td>\n",
       "      <td>24528.3496093750</td>\n",
       "      <td>-5406.4482421875</td>\n",
       "      <td>-14682.1591796875</td>\n",
       "      <td>3649.0617675781</td>\n",
       "      <td>-7673.9218750000</td>\n",
       "      <td>...</td>\n",
       "      <td>4982.6293945312</td>\n",
       "      <td>-13844.1279296875</td>\n",
       "      <td>-2367.4938964844</td>\n",
       "      <td>16342.4062500000</td>\n",
       "      <td>-1967.7547607422</td>\n",
       "      <td>14392.5039062500</td>\n",
       "      <td>-11204.3603515625</td>\n",
       "      <td>2485.1079101562</td>\n",
       "      <td>-2291.1472167969</td>\n",
       "      <td>-32421.9785156250</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 128 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    embed_hadamard_0  embed_hadamard_1  embed_hadamard_2  \\\n",
       "source destination                                                         \n",
       "0      1            18292.3398437500 -10967.8496093750   6717.9316406250   \n",
       "       2             6444.2456054688  -4865.9614257812  18564.5820312500   \n",
       "       3            -5168.5039062500 -15301.2031250000 -14728.5175781250   \n",
       "       4               48.9246826172    135.8364257812   -458.6751708984   \n",
       "       5            -5373.4765625000  21647.4687500000  -9167.5957031250   \n",
       "\n",
       "                    embed_hadamard_3  embed_hadamard_4  embed_hadamard_5  \\\n",
       "source destination                                                         \n",
       "0      1             3907.3608398438  -5661.5107421875 -30468.2500000000   \n",
       "       2            -7174.2094726562    580.8852539062  -1839.9830322266   \n",
       "       3             7321.6181640625  -4063.5617675781  27665.3593750000   \n",
       "       4               10.6340265274     52.8308639526   -162.2333831787   \n",
       "       5            -2164.2231445312  -8469.9921875000  24528.3496093750   \n",
       "\n",
       "                    embed_hadamard_6  embed_hadamard_7  embed_hadamard_8  \\\n",
       "source destination                                                         \n",
       "0      1              -13.9738750458  11908.0644531250   7906.9819335938   \n",
       "       2              788.6420288086  25903.7558593750 -10697.3652343750   \n",
       "       3             1521.4595947266  47032.0156250000  -9536.9345703125   \n",
       "       4              -15.6379966736    428.0025634766    142.1906127930   \n",
       "       5            -5406.4482421875 -14682.1591796875   3649.0617675781   \n",
       "\n",
       "                    embed_hadamard_9  ...  embed_hadamard_118  \\\n",
       "source destination                    ...                       \n",
       "0      1            10222.1806640625  ...   -34283.9414062500   \n",
       "       2           -17187.1054687500  ...   -16710.6914062500   \n",
       "       3            -2820.9868164062  ...   -25174.0312500000   \n",
       "       4               22.5185089111  ...      172.2537536621   \n",
       "       5            -7673.9218750000  ...     4982.6293945312   \n",
       "\n",
       "                    embed_hadamard_119  embed_hadamard_120  \\\n",
       "source destination                                           \n",
       "0      1                499.7468566895    -2004.0174560547   \n",
       "       2             -13426.5742187500    21930.8105468750   \n",
       "       3               1707.0655517578   -12556.5927734375   \n",
       "       4                 40.4835968018     -294.1731567383   \n",
       "       5             -13844.1279296875    -2367.4938964844   \n",
       "\n",
       "                    embed_hadamard_121  embed_hadamard_122  \\\n",
       "source destination                                           \n",
       "0      1              11522.1865234375     5473.8525390625   \n",
       "       2               9325.2441406250    54177.2070312500   \n",
       "       3             -32706.7480468750    18651.5722656250   \n",
       "       4                -54.0811691284      380.0866394043   \n",
       "       5              16342.4062500000    -1967.7547607422   \n",
       "\n",
       "                    embed_hadamard_123  embed_hadamard_124  \\\n",
       "source destination                                           \n",
       "0      1              12427.4824218750     6022.7465820312   \n",
       "       2             -21161.0039062500     7436.7763671875   \n",
       "       3              58582.0546875000     6566.5454101562   \n",
       "       4                269.6816711426     -214.1652374268   \n",
       "       5              14392.5039062500   -11204.3603515625   \n",
       "\n",
       "                    embed_hadamard_125  embed_hadamard_126  embed_hadamard_127  \n",
       "source destination                                                              \n",
       "0      1              -2419.7011718750     2087.1096191406   -22620.0351562500  \n",
       "       2              -1156.2996826172    -3662.7375488281   -47154.8203125000  \n",
       "       3               2471.0539550781    -4822.8417968750   -54639.6796875000  \n",
       "       4                 11.5816688538       18.2915687561     -381.3621826172  \n",
       "       5               2485.1079101562    -2291.1472167969   -32421.9785156250  \n",
       "\n",
       "[5 rows x 128 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show result\n",
    "pair_features_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ee198983",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free-up memory\n",
    "del embedding_df\n",
    "del samples_df\n",
    "del source_vectors\n",
    "del destination_vectors\n",
    "del hadamard_product"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a14c4a",
   "metadata": {},
   "source": [
    "### Domain Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e929b3c",
   "metadata": {},
   "source": [
    "Here is a palette of candidate features that were considered for the link prediction model:\n",
    "- **Radius of Gyration**: A measure of a user's typical travel radius, distinguishing \"stayers\" (small radius) from \"travelers\" (large radius).\n",
    "- **Total Check-in Count**: The total number of check-ins for a user, indicating their overall activity level.\n",
    "- **Unique Locations Count**: The number of distinct locations a user has visited, indicating their tendency to explore.\n",
    "- **Jaccard Similarity of Visited Locations**: The overlap in the set of unique locations visited by two users, measuring shared lifestyle and interests.\n",
    "- **Co-check-in Count**: The number of times two users checked into the same location within a short time window (e.g., 1 hour), indicating possible real-world interaction.\n",
    "- **Haversine Distance between User Centroids**: The geographic distance between the average location (centroid) of all check-ins for each user.\n",
    "- **Haversine Distance between Inferred Home Locations**: The geographic distance between the inferred \"home\" location of each user.\n",
    "- **Explicit Topological Features**: Common Neighbors, Jaccard Coefficient, Adamic-Adar, Resource Allocation, Preferential Attachment\n",
    "\n",
    "The first selection will be based on covering each domain of interaction with one feature that is thought to be the best:\n",
    "- **Radius of Gyration**: Analyzed in the EDA, represent the travelers vs. stayers concept and is an indicator of heterophily.\n",
    "- **Haversine Distance between Inferred Home Location**: the core finding in the EDA, represent the geographic proximity and has already been proved a strong indicator of homophily.\n",
    "- **Jaccard Similarity of Visited Locations**: we didn't explicitly explore this in the EDA, but could strongly represent the shared habits for a pair of users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c8e19ce6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Original Dataset] - Number of users with at least one check-in: 107092 (54.47%)\n",
      "[Time Train Dataset] - Number of users with at least one check-in: 63526 (32.31%)\n",
      "[Volume Train Dataset] - Number of users with at least one check-in: 94019 (47.82%)\n"
     ]
    }
   ],
   "source": [
    "checkins_per_user = checkins_df['user'].value_counts()\n",
    "print(f'[Original Dataset] - Number of users with at least one check-in: {len(checkins_per_user)} ({len(checkins_per_user)/G.number_of_nodes()*100:.2f}%)')\n",
    "checkins_per_user = t_train_df['user'].value_counts()\n",
    "print(f'[Time Train Dataset] - Number of users with at least one check-in: {len(checkins_per_user)} ({len(checkins_per_user)/G.number_of_nodes()*100:.2f}%)')\n",
    "checkins_per_user = v_train_df['user'].value_counts()\n",
    "print(f'[Volume Train Dataset] - Number of users with at least one check-in: {len(checkins_per_user)} ({len(checkins_per_user)/G.number_of_nodes()*100:.2f}%)')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea95ef45",
   "metadata": {},
   "source": [
    "#### Pair-Wise Radius of Gyration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "994ea39d",
   "metadata": {},
   "source": [
    "We need to recalculate the Radius of Gyration using only the check-ins before the cutoff date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "90cdc505",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Radius of Gyration with only check-ins before cutoff\n",
    "# Calculate Mean Centroid\n",
    "users_mean_centroids = t_train_df.groupby('user').agg(\n",
    "    mean_centroid_latitude=('latitude', 'mean'),\n",
    "    mean_centroid_longitude=('longitude', 'mean')\n",
    ")\n",
    "\n",
    "# Merge Mean Centroid with check-ins dataframe\n",
    "checkins_with_centroids = pd.merge(t_train_df, users_mean_centroids, on='user')\n",
    "checkins_with_centroids = checkins_with_centroids[['user', 'latitude', 'longitude', 'mean_centroid_latitude', 'mean_centroid_longitude']]\n",
    "\n",
    "# Zip to obtain coordinates pairs\n",
    "checkins_coords = list(zip(checkins_with_centroids['latitude'], checkins_with_centroids['longitude']))\n",
    "centroid_coords = list(zip(checkins_with_centroids['mean_centroid_latitude'], checkins_with_centroids['mean_centroid_longitude']))\n",
    "\n",
    "# Efficient vectorized Haversine Distance\n",
    "distances = haversine_vector(checkins_coords, centroid_coords)\n",
    "checkins_with_centroids['sq_distance_from_centroid'] = distances**2\n",
    "\n",
    "# Calculate Radius of Gyration and save in a new dataframe\n",
    "radius_of_gyration = checkins_with_centroids.groupby('user')['sq_distance_from_centroid'].mean().apply(np.sqrt).rename('radius_of_gyration_km')\n",
    "radius_of_gyration.index = radius_of_gyration.index.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b56a6e28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "user\n",
       "0    1203.8236124683\n",
       "4     421.4152289762\n",
       "5      49.8019317812\n",
       "9       5.5957428171\n",
       "10      8.4755866231\n",
       "Name: radius_of_gyration_km, dtype: float64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show results\n",
    "radius_of_gyration.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d3772a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the pair-wise feature: Radius of Gyration Absolute Difference\n",
    "samples_df = pd.DataFrame(combined_samples, columns=['source', 'destination'])\n",
    "source_rg_vector = samples_df['source'].map(radius_of_gyration)\n",
    "destination_rg_vector = samples_df['destination'].map(radius_of_gyration)\n",
    "abs_diff_rg = (source_rg_vector - destination_rg_vector).abs()\n",
    "\n",
    "rg_feature_df = pd.DataFrame({\n",
    "    'rg_source': source_rg_vector,\n",
    "    'rg_destination': destination_rg_vector,\n",
    "    'rg_abs_diff': abs_diff_rg\n",
    "})\n",
    "rg_feature_df.index = pd.MultiIndex.from_frame(samples_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "23d5e5e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rg_source          991583\n",
      "rg_destination    1110422\n",
      "rg_abs_diff       1470012\n",
      "dtype: int64 \n",
      "\n",
      "Radius of Gyration absolute difference missing for 1470012 (77.34%) pairs.\n"
     ]
    }
   ],
   "source": [
    "# Check for NaN values\n",
    "print(rg_feature_df.isna().sum(), \"\\n\")\n",
    "\n",
    "rg_na = rg_feature_df.isna().sum().loc['rg_abs_diff']\n",
    "print(f\"Radius of Gyration absolute difference missing for {rg_na} ({rg_na/len(combined_samples)*100:.4}%) pairs.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6001e20b",
   "metadata": {},
   "source": [
    "The **Radius of Gyration (RoG)** was calculated for all users based on their activity before the time-based cutoff date. As expected, and as a direct consequence of the network's growth, only 32.3% of the total users had check-in data within this period.\n",
    "\n",
    "We now need to deal with the presence of numerous missing values due to having many users not active on the platform in this time range. To treat this information as a predictive signal rather than a data problem, we engineered a dedicated categorical feature, `activity_status`, to explicitly describe the activity state of each pair (`Both Active`, `Source Active Only`, etc.). This allows the model to learn distinct patterns for each scenario. With a feature that gives context the the RoG values, all the NaN can now be filled with zeros. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "63468c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill the NaN values\n",
    "rg_feature_df['rg_source'] = rg_feature_df['rg_source'].fillna(0)\n",
    "rg_feature_df['rg_destination'] = rg_feature_df['rg_destination'].fillna(0)\n",
    "\n",
    "rg_feature_df['rg_abs_diff'] = (rg_feature_df['rg_source'] - rg_feature_df['rg_destination']).abs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f6e71624",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>rg_source</th>\n",
       "      <th>rg_destination</th>\n",
       "      <th>rg_abs_diff</th>\n",
       "      <th>activity_status</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>source</th>\n",
       "      <th>destination</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">0</th>\n",
       "      <th>1</th>\n",
       "      <td>1203.8236124683</td>\n",
       "      <td>0.0000000000</td>\n",
       "      <td>1203.8236124683</td>\n",
       "      <td>source_only</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1203.8236124683</td>\n",
       "      <td>0.0000000000</td>\n",
       "      <td>1203.8236124683</td>\n",
       "      <td>source_only</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1203.8236124683</td>\n",
       "      <td>0.0000000000</td>\n",
       "      <td>1203.8236124683</td>\n",
       "      <td>source_only</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1203.8236124683</td>\n",
       "      <td>421.4152289762</td>\n",
       "      <td>782.4083834921</td>\n",
       "      <td>both_have</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1203.8236124683</td>\n",
       "      <td>49.8019317812</td>\n",
       "      <td>1154.0216806871</td>\n",
       "      <td>both_have</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         rg_source  rg_destination     rg_abs_diff  \\\n",
       "source destination                                                   \n",
       "0      1           1203.8236124683    0.0000000000 1203.8236124683   \n",
       "       2           1203.8236124683    0.0000000000 1203.8236124683   \n",
       "       3           1203.8236124683    0.0000000000 1203.8236124683   \n",
       "       4           1203.8236124683  421.4152289762  782.4083834921   \n",
       "       5           1203.8236124683   49.8019317812 1154.0216806871   \n",
       "\n",
       "                   activity_status  \n",
       "source destination                  \n",
       "0      1               source_only  \n",
       "       2               source_only  \n",
       "       3               source_only  \n",
       "       4                 both_have  \n",
       "       5                 both_have  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add categorical feature for context\n",
    "source_is_active = samples_df['source'].isin(radius_of_gyration.index)\n",
    "destination_is_active = samples_df['destination'].isin(radius_of_gyration.index)\n",
    "\n",
    "conditions = [\n",
    "    source_is_active & destination_is_active,\n",
    "    source_is_active & ~destination_is_active,\n",
    "    ~source_is_active & destination_is_active,\n",
    "    ~source_is_active & ~destination_is_active\n",
    "]\n",
    "choices = ['both_have', 'source_only', 'dest_only', 'neither_have']\n",
    "\n",
    "activity_status = np.select(conditions, choices, default='Error')\n",
    "\n",
    "rg_feature_df['activity_status'] = activity_status\n",
    "rg_feature_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "233f4dfb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>rg_source</th>\n",
       "      <th>rg_destination</th>\n",
       "      <th>rg_abs_diff</th>\n",
       "      <th>rg_status_both_have</th>\n",
       "      <th>rg_status_dest_only</th>\n",
       "      <th>rg_status_neither_have</th>\n",
       "      <th>rg_status_source_only</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>source</th>\n",
       "      <th>destination</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">0</th>\n",
       "      <th>1</th>\n",
       "      <td>1203.8236124683</td>\n",
       "      <td>0.0000000000</td>\n",
       "      <td>1203.8236124683</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1203.8236124683</td>\n",
       "      <td>0.0000000000</td>\n",
       "      <td>1203.8236124683</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1203.8236124683</td>\n",
       "      <td>0.0000000000</td>\n",
       "      <td>1203.8236124683</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1203.8236124683</td>\n",
       "      <td>421.4152289762</td>\n",
       "      <td>782.4083834921</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1203.8236124683</td>\n",
       "      <td>49.8019317812</td>\n",
       "      <td>1154.0216806871</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         rg_source  rg_destination     rg_abs_diff  \\\n",
       "source destination                                                   \n",
       "0      1           1203.8236124683    0.0000000000 1203.8236124683   \n",
       "       2           1203.8236124683    0.0000000000 1203.8236124683   \n",
       "       3           1203.8236124683    0.0000000000 1203.8236124683   \n",
       "       4           1203.8236124683  421.4152289762  782.4083834921   \n",
       "       5           1203.8236124683   49.8019317812 1154.0216806871   \n",
       "\n",
       "                    rg_status_both_have  rg_status_dest_only  \\\n",
       "source destination                                             \n",
       "0      1                          False                False   \n",
       "       2                          False                False   \n",
       "       3                          False                False   \n",
       "       4                           True                False   \n",
       "       5                           True                False   \n",
       "\n",
       "                    rg_status_neither_have  rg_status_source_only  \n",
       "source destination                                                 \n",
       "0      1                             False                   True  \n",
       "       2                             False                   True  \n",
       "       3                             False                   True  \n",
       "       4                             False                  False  \n",
       "       5                             False                  False  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# One-hot encoding categorical feature\n",
    "rg_feature_df = pd.get_dummies(rg_feature_df, columns=['activity_status'], prefix='rg_status')\n",
    "rg_feature_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fd8b435d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add results to the complete pair feature dataframe\n",
    "pair_features_df = pair_features_df.join(rg_feature_df.drop(columns=['rg_source', 'rg_destination']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0d66db16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free-Up Memory\n",
    "del users_mean_centroids\n",
    "del checkins_with_centroids\n",
    "del checkins_coords\n",
    "del centroid_coords\n",
    "del distances\n",
    "del radius_of_gyration\n",
    "del samples_df\n",
    "del source_rg_vector\n",
    "del destination_rg_vector\n",
    "del abs_diff_rg\n",
    "del rg_feature_df\n",
    "del source_is_active\n",
    "del destination_is_active\n",
    "del activity_status\n",
    "del conditions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf83b3a",
   "metadata": {},
   "source": [
    "#### Haversine Distance between Inferred Home Locations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed5c1a54",
   "metadata": {},
   "source": [
    "Again, all the calculations needs to be redone using only the check-ins before the cutoff date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0f6aa385",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reuse code from EDA\n",
    "inferred_home_df = t_train_df.groupby('user').agg(\n",
    "    median_centroid_latitude=('latitude', 'median'),\n",
    "    median_centroid_longitude=('longitude', 'median')\n",
    ")\n",
    "\n",
    "# Heuristic to infer Home Locations\n",
    "home_hours = (t_train_df['check-in_datetime'].dt.hour >= 21) | (t_train_df['check-in_datetime'].dt.hour < 7)\n",
    "\n",
    "# The grid will have 25x25 Kilometers cells\n",
    "lat_step, lon_step = 0.25, 0.25\n",
    "t_train_df['lat_bin'] = (t_train_df['latitude'] / lat_step).astype(int)\n",
    "t_train_df['lon_bin'] = (t_train_df['longitude'] / lon_step).astype(int)\n",
    "\n",
    "# Find the most visited grid cell for each user during home hours\n",
    "home_cells = t_train_df[home_hours]\\\n",
    "    .groupby(['user', 'lat_bin', 'lon_bin'])\\\n",
    "    .size()\\\n",
    "    .reset_index(name='count')\\\n",
    "    .sort_values('count', ascending=False)\\\n",
    "    .drop_duplicates(subset='user')\\\n",
    "    .set_index('user')\\\n",
    "    [['lat_bin', 'lon_bin']]\\\n",
    "    .rename(columns={'lat_bin': 'home_lat_bin', 'lon_bin': 'home_lon_bin'})\n",
    "\n",
    "# Chain operations to calculate the centroid of check-ins within each user's home cell\n",
    "home_cell_centroids = (\n",
    "    t_train_df.join(home_cells, on='user')\n",
    "    .dropna(subset=['home_lat_bin'])\n",
    "    .query('lat_bin == home_lat_bin and lon_bin == home_lon_bin')\n",
    "    .groupby('user')\n",
    "    .agg(\n",
    "        home_cell_centroid_lat=('latitude', 'median'),\n",
    "        home_cell_centroid_lon=('longitude', 'median')\n",
    "    )\n",
    ")\n",
    "\n",
    "inferred_home_df = inferred_home_df.join(home_cell_centroids)\n",
    "inferred_home_df.index = inferred_home_df.index.astype(int)\n",
    "inferred_home_df = inferred_home_df[['home_cell_centroid_lat', 'home_cell_centroid_lon']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0045af80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "home_cell_centroid_lat    10739\n",
       "home_cell_centroid_lon    10739\n",
       "dtype: int64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inferred_home_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "955c9788",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_df = pd.DataFrame(combined_samples, columns=[\"source\", \"destination\"])\n",
    "\n",
    "samples_df = samples_df.join(inferred_home_df, on=\"source\")\n",
    "samples_df = samples_df.rename(columns={\n",
    "    \"home_cell_centroid_lat\": \"source_home_lat\",\n",
    "    \"home_cell_centroid_lon\": \"source_home_lon\",\n",
    "})\n",
    "\n",
    "samples_df = samples_df.join(inferred_home_df, on=\"destination\")\n",
    "samples_df = samples_df.rename(columns={\n",
    "    \"home_cell_centroid_lat\": \"dest_home_lat\",\n",
    "    \"home_cell_centroid_lon\": \"dest_home_lon\",\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "10fd6da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_df.index = pd.MultiIndex.from_frame(samples_df[[\"source\", \"destination\"]])\n",
    "samples_df = samples_df.drop(columns=[\"source\", \"destination\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "50949ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_home_coords = list(zip(samples_df['source_home_lat'], samples_df['source_home_lon']))\n",
    "dest_home_coords = list(zip(samples_df['dest_home_lat'], samples_df['dest_home_lon']))\n",
    "\n",
    "distances = haversine_vector(source_home_coords, dest_home_coords)\n",
    "samples_df['home_distance_km'] = distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f1abb90b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source_home_lat     1060573\n",
      "source_home_lon     1060573\n",
      "dest_home_lat       1201301\n",
      "dest_home_lon       1201301\n",
      "home_distance_km    1539456\n",
      "dtype: int64 \n",
      "\n",
      "Home Distance missing for 1539456 (81.0%) pairs.\n"
     ]
    }
   ],
   "source": [
    "# Check for NaN values\n",
    "print(samples_df.isna().sum(), \"\\n\")\n",
    "\n",
    "home_dist_na = samples_df.isna().sum().loc['home_distance_km']\n",
    "print(f\"Home Distance missing for {home_dist_na} ({home_dist_na/len(combined_samples)*100:.4}%) pairs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "387a59b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_has_home = samples_df['source_home_lat'].notna()\n",
    "dest_has_home = samples_df['dest_home_lat'].notna()\n",
    "\n",
    "conditions = [\n",
    "    source_has_home & dest_has_home,\n",
    "    source_has_home & ~dest_has_home,\n",
    "    ~source_has_home & dest_has_home,\n",
    "    ~source_has_home & ~dest_has_home\n",
    "]\n",
    "\n",
    "choices = ['both_have', 'source_only', 'dest_only', 'neither_have']\n",
    "samples_df['home_status'] = np.select(conditions, choices, default='unknown')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0374771a",
   "metadata": {},
   "source": [
    "The `home_distance_km` column now contains the calculated distances for pairs where both users had an inferred home, and NaN for all other pairs. A simple imputation with zero would be misleading, as it would imply that users with no inferred home live at the same location. Therefore, a more neutral and statistically robust approach is chosen: the missing distance values are imputed using the **median** of all the calculated distances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "22942e07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Median Home Distance:  832.7161963358135\n"
     ]
    }
   ],
   "source": [
    "# Impute the missing home distance values with the median of the distances\n",
    "# TODO: could also try to impute with a very large distance\n",
    "median_distance = samples_df['home_distance_km'].median()\n",
    "samples_df['home_distance_km'] = samples_df['home_distance_km'].fillna(median_distance)\n",
    "\n",
    "print(\"Median Home Distance: \", median_distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b16df100",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encoding categorical feature\n",
    "samples_df = pd.get_dummies(samples_df, columns=['home_status'], prefix='home_status')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2d41fc17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add results to the complete pair feature dataframe\n",
    "pair_features_df = pair_features_df.join(samples_df.drop(columns=['source_home_lat', 'source_home_lon', 'dest_home_lat', 'dest_home_lon']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "db694820",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free-Up Memory\n",
    "del inferred_home_df\n",
    "del home_cells\n",
    "del home_hours\n",
    "del home_cell_centroids\n",
    "del samples_df\n",
    "del source_home_coords\n",
    "del dest_home_coords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778e302a",
   "metadata": {},
   "source": [
    "#### Jaccard Similarity of Visited Locations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67df0cd4",
   "metadata": {},
   "source": [
    "This is a new introduced feature that we didn't considered in the EDA phase. The goal here is to measure shared lifestyle and interests. We hypothesize that users who frequent the same set of locations, are more likely to be friends. The Jaccard Similarity of their visited location sets provides an intuitive measure of this behavioral overlap.\n",
    "\n",
    "The similarity alone can be a weak signal if it's based on few shared location, but could be a strong one if it's based on many shared locations.\n",
    "The initial idea was to weight the similarity with the visited location count, but having a nonlinear model like GBM we can give the model the raw components and let it discover the complex interactions itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5852704e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary of users - visited locations\n",
    "user_location_sets = t_train_df.groupby('user')['location_id'].apply(set)\n",
    "user_location_dict = user_location_sets.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c6d7f504",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Location Overlap function\n",
    "def _location_overlap(set1, set2):\n",
    "    intersection_size = len(set1.intersection(set2))\n",
    "    union_size = len(set1.union(set2))\n",
    "\n",
    "    if union_size == 0:\n",
    "        return 0.0, intersection_size, union_size\n",
    "\n",
    "    return (intersection_size / union_size), intersection_size, union_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a1fd7acf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>jaccard_similarity_locations</th>\n",
       "      <th>intersect_locations</th>\n",
       "      <th>unions_locations</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>source</th>\n",
       "      <th>destination</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">0</th>\n",
       "      <th>1</th>\n",
       "      <td>0.0000000000</td>\n",
       "      <td>0</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0000000000</td>\n",
       "      <td>0</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0000000000</td>\n",
       "      <td>0</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0294117647</td>\n",
       "      <td>3</td>\n",
       "      <td>102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0000000000</td>\n",
       "      <td>0</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    jaccard_similarity_locations  intersect_locations  \\\n",
       "source destination                                                      \n",
       "0      1                            0.0000000000                    0   \n",
       "       2                            0.0000000000                    0   \n",
       "       3                            0.0000000000                    0   \n",
       "       4                            0.0294117647                    3   \n",
       "       5                            0.0000000000                    0   \n",
       "\n",
       "                    unions_locations  \n",
       "source destination                    \n",
       "0      1                          34  \n",
       "       2                          34  \n",
       "       3                          34  \n",
       "       4                         102  \n",
       "       5                          57  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate Jaccard Similarity using list comprehensions\n",
    "samples_df = pd.DataFrame(combined_samples, columns=[\"source\", \"destination\"])\n",
    "\n",
    "locations_overlap_data = [\n",
    "    _location_overlap(user_location_dict.get(source, set()), user_location_dict.get(destination, set()))\n",
    "    for source, destination in zip(samples_df['source'], samples_df['destination'])\n",
    "]\n",
    "\n",
    "jaccard, intersect_sizes, union_sizes = zip(*locations_overlap_data)\n",
    "\n",
    "jaccard_feature_df = pd.DataFrame({\n",
    "    'jaccard_similarity_locations': jaccard,\n",
    "    'intersect_locations': intersect_sizes,\n",
    "    'unions_locations': union_sizes\n",
    "})\n",
    "\n",
    "jaccard_feature_df.index = pd.MultiIndex.from_frame(samples_df)\n",
    "\n",
    "jaccard_feature_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0cc3d2b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "jaccard_similarity_locations    0\n",
       "intersect_locations             0\n",
       "unions_locations                0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for NaNs\n",
    "jaccard_feature_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "851781f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 110675 (5.82%) pairs with locations similarity >0.\n",
      "There are 1789979 (94.18%) pairs with locations similarity =0.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>jaccard_similarity_locations</th>\n",
       "      <th>intersect_locations</th>\n",
       "      <th>unions_locations</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1900654.0000000000</td>\n",
       "      <td>1900654.0000000000</td>\n",
       "      <td>1900654.0000000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.0019025140</td>\n",
       "      <td>0.2376182093</td>\n",
       "      <td>56.9806692854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.0141596191</td>\n",
       "      <td>2.1730818325</td>\n",
       "      <td>125.3736290873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.0000000000</td>\n",
       "      <td>0.0000000000</td>\n",
       "      <td>0.0000000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.0000000000</td>\n",
       "      <td>0.0000000000</td>\n",
       "      <td>0.0000000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.0000000000</td>\n",
       "      <td>0.0000000000</td>\n",
       "      <td>14.0000000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.0000000000</td>\n",
       "      <td>0.0000000000</td>\n",
       "      <td>60.0000000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.0000000000</td>\n",
       "      <td>445.0000000000</td>\n",
       "      <td>2988.0000000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       jaccard_similarity_locations  intersect_locations   unions_locations\n",
       "count            1900654.0000000000   1900654.0000000000 1900654.0000000000\n",
       "mean                   0.0019025140         0.2376182093      56.9806692854\n",
       "std                    0.0141596191         2.1730818325     125.3736290873\n",
       "min                    0.0000000000         0.0000000000       0.0000000000\n",
       "25%                    0.0000000000         0.0000000000       0.0000000000\n",
       "50%                    0.0000000000         0.0000000000      14.0000000000\n",
       "75%                    0.0000000000         0.0000000000      60.0000000000\n",
       "max                    1.0000000000       445.0000000000    2988.0000000000"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Overview of the results\n",
    "sim_not_zero = len(jaccard_feature_df[jaccard_feature_df['jaccard_similarity_locations'] > 0])\n",
    "sim_zero = len(jaccard_feature_df[jaccard_feature_df['jaccard_similarity_locations'] == 0])\n",
    "\n",
    "print(f\"There are {sim_not_zero} ({sim_not_zero/len(jaccard_feature_df)*100:.2f}%) pairs with locations similarity >0.\")\n",
    "print(f\"There are {sim_zero} ({sim_zero/len(jaccard_feature_df)*100:.2f}%) pairs with locations similarity =0.\")\n",
    "\n",
    "jaccard_feature_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa92574",
   "metadata": {},
   "source": [
    "As was to be expected the vast majority of pairs don't have similar visited location, because the data is sparse from all over the world."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9040d1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add results to the complete pair feature dataframe\n",
    "pair_features_df = pair_features_df.join(jaccard_feature_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "37112da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free-Up Memory\n",
    "del jaccard_feature_df\n",
    "del locations_overlap_data\n",
    "del samples_df\n",
    "del user_location_sets\n",
    "del user_location_dict\n",
    "del jaccard, intersect_sizes, union_sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff58aa37",
   "metadata": {},
   "source": [
    "### Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "42ef7e47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1900654, 141)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pair_features_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c71773df",
   "metadata": {},
   "outputs": [],
   "source": [
    "pair_features_df['is_friend'] = samples_labels\n",
    "pair_features_df.to_parquet(\"data/engineered_features/final_modeling_dataset.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a545cc2",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78b190e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import optuna\n",
    "import lightgbm as lgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79eea9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "modeling_df = pd.read_parquet(\"data/engineered_features/final_modeling_dataset.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d041e478",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1900654, 142)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modeling_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f5c67965",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (1520523, 141)\n",
      "X_test shape:  (380131, 141)\n"
     ]
    }
   ],
   "source": [
    "# Final Train/Test Split\n",
    "X = modeling_df.drop(columns=['is_friend'])\n",
    "y = modeling_df['is_friend']\n",
    "\n",
    "# Perform the 80/20 split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"X_test shape:  {X_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6661e1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "del modeling_df, X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7dd5ab9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subsample Data for Hyperparameter Tuning\n",
    "X_train_sample, _, y_train_sample, _ = train_test_split(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    train_size=0.1, # 10%\n",
    "    random_state=42,\n",
    "    stratify=y_train\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0757c920",
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_dataset_sample = lgbm.Dataset(X_train_sample, label=y_train_sample)\n",
    "\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        # Forest params\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 200, 1000),\n",
    "\n",
    "        # Tree params\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 12),\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 8, 4096),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "        'subsample': trial.suggest_float('subsample', 0.7, 0.9), # bagging fraction\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 0.9), # feature subset fraction\n",
    "\n",
    "        # Model settings\n",
    "        'objective': 'binary',\n",
    "        'metric': 'auc',\n",
    "        'random_state': 42,\n",
    "        'n_jobs': -1,\n",
    "        'verbose': 1\n",
    "    }\n",
    "\n",
    "\n",
    "    cv_results = lgbm.cv(\n",
    "        params=params,\n",
    "        train_set=lgbm_dataset_sample,\n",
    "        num_boost_round=params['n_estimators'],\n",
    "        nfold=5,\n",
    "        stratified=True,\n",
    "        seed=42,\n",
    "        callbacks=[lgbm.early_stopping(10, verbose=True)]\n",
    "    )\n",
    "\n",
    "    best_score = max(cv_results['valid auc-mean']) # type: ignore\n",
    "\n",
    "    return best_score\n",
    "\n",
    "\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=50, show_progress_bar=True, gc_after_trial=True)\n",
    "\n",
    "best_params = study.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "823cf0bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8759919906092769"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "study.trials[0].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "13ed5ff1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FrozenTrial(number=15, state=1, values=[0.8750329709173551], datetime_start=datetime.datetime(2025, 7, 13, 13, 19, 42, 240357), datetime_complete=datetime.datetime(2025, 7, 13, 13, 22, 7, 194358), params={'n_estimators': 375, 'max_depth': 11, 'num_leaves': 1133, 'learning_rate': 0.03877486168812111, 'subsample': 0.8460268736474088, 'colsample_bytree': 0.6915663877398335}, user_attrs={}, system_attrs={}, intermediate_values={}, distributions={'n_estimators': IntDistribution(high=1000, log=False, low=200, step=1), 'max_depth': IntDistribution(high=12, log=False, low=3, step=1), 'num_leaves': IntDistribution(high=4096, log=False, low=8, step=1), 'learning_rate': FloatDistribution(high=0.3, log=True, low=0.01, step=None), 'subsample': FloatDistribution(high=0.9, log=False, low=0.7, step=None), 'colsample_bytree': FloatDistribution(high=0.9, log=False, low=0.6, step=None)}, trial_id=15, value=None)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_score = study.best_value\n",
    "tolerance = 0.01\n",
    "\n",
    "candidates = [\n",
    "    trial for trial in study.trials\n",
    "    if trial.state == optuna.trial.TrialState.COMPLETE and\n",
    "       trial.values[0] >= best_score - tolerance\n",
    "]\n",
    "\n",
    "min(candidates, key=lambda t: t.params[\"n_estimators\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-env-3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
