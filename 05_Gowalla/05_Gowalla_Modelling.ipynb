{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1867763",
   "metadata": {},
   "source": [
    "# Gowalla - Modelling\n",
    "After looking at the data in the EDA, it's time to build our prediction model. The main goal is to predict if two users will be friends based on their behavior and their place in the network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54098aa9",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96c9d910",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "from haversine import haversine_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9e88123",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8672b6ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "edges_path = 'data/Gowalla_edges.txt'\n",
    "\n",
    "# Data loading\n",
    "G = nx.read_edgelist(edges_path, nodetype=int)\n",
    "checkins_df = pd.read_parquet('data/EDA_output/checkins.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49632a22",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef635e1c",
   "metadata": {},
   "source": [
    "Initially, a simple random 80/20 split of the edges was considered for creating the training and test sets. However, this approach was identified as methodologically flawed for a temporal dataset. A random split could create a scenario where the model is trained on check-in features from 2010 to predict a link that was held out for the test set, even if that link represents a friendship from 2009. This constitutes a form of data leakage, where information from the future is used to predict the past, leading to an overly optimistic and unrealistic evaluation of the model's performance.\"\n",
    "\n",
    "To avoid this temporal leakage and address the limitations of the data, we reframe the prediction task. Given the absence of friendship formation timestamps that makes true temporal prediction impossible, we *shift from predicting link formation to predicting link existence within the final network snapshot*.\n",
    "\n",
    "> Can we use a user's early activity to effectively distinguish between user pairs who are friends and those who are not in the final, complete social network?\n",
    "\n",
    "In this new objective we accept **structural leakage** by using the final graph for embeddings, but we explicitly define the task around this by predicting the *final state*, not the *formation*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5897528",
   "metadata": {},
   "source": [
    "### Feature Dataset Train/Test Split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7fcbad",
   "metadata": {},
   "source": [
    "We proceed to make the 80/20 split of the check-ins. We have two alternative:\n",
    "- Split by volume of check-ins (e.g. train set will have 80% of check-ins)\n",
    "- Split by time passed (e.g train set will have check-ins that appears in the first 80% of time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f585fdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check-ins cutoff at:  2010-06-20 00:33:12.400000+00:00\n",
      "Train set size: 2939173 - Corresponding percentage of volume: 45.6\n",
      "Test set size:  3503690 - Corresponding percentage of volume: 54.4\n"
     ]
    }
   ],
   "source": [
    "# Split the check-ins dataframe by time\n",
    "min_ts = checkins_df['check-in_datetime'].min()\n",
    "max_ts = checkins_df['check-in_datetime'].max()\n",
    "\n",
    "time_diff = max_ts - min_ts\n",
    "\n",
    "time_cutoff = min_ts + pd.to_timedelta(0.8 * time_diff)\n",
    "\n",
    "t_train_df = checkins_df[checkins_df['check-in_datetime'] < time_cutoff]\n",
    "t_test_df = checkins_df[checkins_df['check-in_datetime'] >= time_cutoff]\n",
    "\n",
    "print(\"Check-ins cutoff at: \", time_cutoff)\n",
    "print(f\"Train set size: {len(t_train_df)} - Corresponding percentage of volume: {len(t_train_df)/len(checkins_df)*100:.1f}\")\n",
    "print(f\"Test set size:  {len(t_test_df)} - Corresponding percentage of volume: {len(t_test_df)/len(checkins_df)*100:.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0f1e30e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check-ins cutoff at:  2010-09-14 14:24:41+00:00\n",
      "Train set size: 5154291 - Corresponding percentage of time: 93.8\n",
      "Test set size:  1288572 - Corresponding percentage of time: 6.2\n"
     ]
    }
   ],
   "source": [
    "# Split the check-ins dataframe by volume\n",
    "volume_cutoff = math.ceil(len(checkins_df) * 0.80)\n",
    "\n",
    "checkins_df = checkins_df.sort_values(by=['check-in_datetime'])\n",
    "\n",
    "v_train_df = checkins_df.iloc[:volume_cutoff]\n",
    "v_test_df = checkins_df.iloc[volume_cutoff:]\n",
    "\n",
    "volume_cutoff_ts = checkins_df.iloc[volume_cutoff]['check-in_datetime']\n",
    "\n",
    "print(\"Check-ins cutoff at: \", volume_cutoff_ts)\n",
    "print(f\"Train set size: {len(v_train_df)} - Corresponding percentage of time: {((volume_cutoff_ts - min_ts) / time_diff) * 100:.1f}\")\n",
    "print(f\"Test set size:  {len(v_test_df)} - Corresponding percentage of time: {((max_ts - volume_cutoff_ts) / time_diff) * 100:.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ddd4050",
   "metadata": {},
   "source": [
    "Either way we obtain an overall imbalanced split, and either options have pros and cons:\n",
    "- Option \"Time\": This is the most honest representation of a real-world temporal prediction task, but the features built from this sparser data might be weak.\n",
    "- Option \"Volume\": The features built will be much stronger and the model will surely perform better, but the separation between past and future is tiny.\n",
    "\n",
    "As the goal of the notebook is learning, we proceed with the time-based split."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e13a1d9",
   "metadata": {},
   "source": [
    "### Negative Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c8ece56",
   "metadata": {},
   "source": [
    "To create a balanced dataset we take an equal number of positive and negative samples. To make the training set challenging, we use a mixed strategy:\n",
    "- 70% of negative samples are \"hard\" negatives generated with random walks with a path distance of 2.\n",
    "- 20% are \"medium\" negatives generated with random walks with a path distance of 4, representing users who are further apart but still connected.\n",
    "- 10% are \"easy\" random negatives, to ensure the model learns the global structure of the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c3d860a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Negative Sampling Generation Functions\n",
    "def _random_walk_hard_negative(graph, count, walk_distance=2, max_attempts=100):\n",
    "    \"\"\"Generates a batch of hard negative samples.\"\"\"\n",
    "    if walk_distance < 1:\n",
    "        raise ValueError(\"walk_distance must be >= 1\")\n",
    "\n",
    "    edges = list(graph.edges())\n",
    "    if not edges:\n",
    "        return []\n",
    "\n",
    "    generated_samples = []\n",
    "    for _ in range(count):\n",
    "        for _ in range(max_attempts):\n",
    "            u, current_node = random.choice(edges)\n",
    "\n",
    "            # Make hops\n",
    "            for _ in range(walk_distance - 1):\n",
    "                neighbors = list(graph.neighbors(current_node))\n",
    "                if not neighbors:\n",
    "                    current_node = None\n",
    "                    break\n",
    "                current_node = random.choice(neighbors)\n",
    "\n",
    "            if current_node is None:\n",
    "                continue\n",
    "\n",
    "            v = current_node\n",
    "\n",
    "            # Validate edge and add to the batch\n",
    "            if v != u and not graph.has_edge(u, v):\n",
    "                generated_samples.append((u, v))\n",
    "                break\n",
    "\n",
    "    return generated_samples\n",
    "\n",
    "def _random_negative_sampling(graph, count, max_attempts=100):\n",
    "    \"\"\"Generates a batch of random negative samples.\"\"\"\n",
    "    nodes = list(graph.nodes())\n",
    "    if len(nodes) < 2:\n",
    "        return []\n",
    "\n",
    "    generated_samples = []\n",
    "    for _ in range(count):\n",
    "        for _ in range(max_attempts):\n",
    "            u, v = random.sample(nodes, 2)\n",
    "            if not graph.has_edge(u, v):\n",
    "                generated_samples.append((u, v))\n",
    "                break\n",
    "\n",
    "    return generated_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b4c46f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main Negative Sampling Function to combine different methods\n",
    "def generate_negative_samples_set(\n",
    "    graph,\n",
    "    sampling_functions,\n",
    "    sampling_weights,\n",
    "    ratio=1.0,\n",
    "    seed=None\n",
    "):\n",
    "    if seed is not None:\n",
    "        random.seed(seed)\n",
    "        np.random.seed(seed)\n",
    "\n",
    "    if len(sampling_functions) != len(sampling_weights):\n",
    "        raise ValueError(\"sampling_functions and sampling_weights must be of the same length\")\n",
    "\n",
    "    total_neg_samples = int(len(graph.edges) * ratio)\n",
    "\n",
    "    weights = np.array(sampling_weights, dtype=float)\n",
    "    weights /= weights.sum()\n",
    "    samples_for_each_function = np.random.multinomial(total_neg_samples, weights)\n",
    "\n",
    "    negative_samples = set()\n",
    "\n",
    "    for func, count in zip(sampling_functions, samples_for_each_function):\n",
    "        if count == 0:\n",
    "            continue\n",
    "\n",
    "        new_samples = func(graph, count)\n",
    "        negative_samples.update(new_samples)\n",
    "\n",
    "    return negative_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ed9d1a56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 973408 (102.43%) negative samples.\n"
     ]
    }
   ],
   "source": [
    "# Generate Negative Samples\n",
    "positive_samples = list(G.edges)\n",
    "\n",
    "sampling_functions = [\n",
    "    lambda g, c: _random_negative_sampling(g, c),\n",
    "    lambda g, c: _random_walk_hard_negative(g, c, walk_distance=4),\n",
    "    lambda g, c: _random_walk_hard_negative(g, c, walk_distance=2)\n",
    "]\n",
    "\n",
    "sampling_weights = [0.1, 0.2, 0.7]\n",
    "\n",
    "negative_samples = generate_negative_samples_set(\n",
    "    graph=G,\n",
    "    sampling_functions=sampling_functions,\n",
    "    sampling_weights=sampling_weights,\n",
    "    ratio=1.1, # Allow to compensate duplicates\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "print(f\"Generated {len(negative_samples)} ({len(negative_samples)/len(positive_samples)*100:.2f}%) negative samples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "04954f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Truncate the exceeding part\n",
    "negative_samples = random.sample(list(negative_samples), len(positive_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6c2c510a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine and create labels\n",
    "combined_samples = positive_samples + negative_samples\n",
    "samples_labels = [1] * len(positive_samples) + [0] * len(negative_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5fda52e",
   "metadata": {},
   "source": [
    "### Node Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dbf3f9d",
   "metadata": {},
   "source": [
    "The goal of **Node Embedding** is to learn a *dense vector representation* for each user that captures their neighborhood and role in the network. This approach allows the model to automatically learn features from the graph topology, serving as a powerful alternative to manual feature engineering with standard measures (e.g., Common Neighbors).\n",
    "\n",
    "We chose the **Node2Vec** algorithm for this task. An initial test with a standard Python implementation of Node2Vec proved to be extremely slow. The first phase of the algorithm, *\"computing transition probabilities\"* was single-threaded and was projected to take several hours. A Stack Overflow discussion<sup>[[1][1]]</sup> highlighted this common bottleneck and pointed towards more performant, specialized libraries.\n",
    "\n",
    "[1]: https://stackoverflow.com/questions/60276191/is-there-any-way-to-make-node2vec-faster\n",
    "\n",
    "Based on this research, we switched to **GRAPE**<sup>[[2][2]]</sup>, a fast graph processing and embedding library. GRAPE is written in *Rust* and *Python*, developed primarily at the University of Milan, and is designed for speed and scalability on massive graphs. It parallelizes the entire Node2Vec process, including the pre-computation step, which dramatically reduces the training time.  \n",
    "While the library's documentation was found to be in a raw state, we were able to successfully implement it for our purposes.\n",
    "\n",
    "[2]: https://github.com/AnacletoLAB/grape\n",
    "\n",
    "To make our workflow efficient and reproducible, we performed this computationally expensive step in a separate script (`train_embeddings.py`) and saved the resulting embeddings to a file. The model was configured with a neutral baseline set of hyperparameters:\n",
    "- `embedding_size` (dimensions): 128\n",
    "- `walk_length` (length of each walk): 80\n",
    "- `iterations` (number of walks per node): 10\n",
    "- `return_weight` ($p = 1 / \\text{return\\_weight}$): 1.0\n",
    "- `explore_weight` ($q = 1 / \\text{explore\\_weight}$): 1.0\n",
    "\n",
    "In the Node2Vec framework `return_weight` and `explore_weight` set to `1.0` corresponds to the parameters `p` and `q`, which removes any search bias and makes the algorithm equivalent to the **DeepWalk** algorithm.\n",
    "\n",
    "The training process, which ran over *30 epochs*, required *35 minutes* to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "22834935",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Embeddings\n",
    "embedding_path = \"embeddings/gowalla_node2vec_embeddings.parquet\"\n",
    "embedding_df = pd.read_parquet(embedding_path)\n",
    "embedding_df.index = embedding_df.index.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3d817e3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>118</th>\n",
       "      <th>119</th>\n",
       "      <th>120</th>\n",
       "      <th>121</th>\n",
       "      <th>122</th>\n",
       "      <th>123</th>\n",
       "      <th>124</th>\n",
       "      <th>125</th>\n",
       "      <th>126</th>\n",
       "      <th>127</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-4.028883</td>\n",
       "      <td>-2.877670</td>\n",
       "      <td>5.043583</td>\n",
       "      <td>-0.282113</td>\n",
       "      <td>9.059772</td>\n",
       "      <td>3.420488</td>\n",
       "      <td>-2.221864</td>\n",
       "      <td>-0.936862</td>\n",
       "      <td>4.680723</td>\n",
       "      <td>0.022812</td>\n",
       "      <td>...</td>\n",
       "      <td>1.390288</td>\n",
       "      <td>-2.516412</td>\n",
       "      <td>6.060315</td>\n",
       "      <td>-0.654452</td>\n",
       "      <td>-0.875411</td>\n",
       "      <td>-1.368012</td>\n",
       "      <td>0.509950</td>\n",
       "      <td>-1.803684</td>\n",
       "      <td>4.541414</td>\n",
       "      <td>0.619863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-243.156174</td>\n",
       "      <td>-22.406778</td>\n",
       "      <td>8.421268</td>\n",
       "      <td>150.557510</td>\n",
       "      <td>-96.713753</td>\n",
       "      <td>138.841431</td>\n",
       "      <td>-76.178436</td>\n",
       "      <td>-37.150448</td>\n",
       "      <td>77.478683</td>\n",
       "      <td>102.910446</td>\n",
       "      <td>...</td>\n",
       "      <td>106.663269</td>\n",
       "      <td>32.469856</td>\n",
       "      <td>66.770645</td>\n",
       "      <td>84.003662</td>\n",
       "      <td>107.856476</td>\n",
       "      <td>-17.861546</td>\n",
       "      <td>-122.073204</td>\n",
       "      <td>-141.799301</td>\n",
       "      <td>8.232184</td>\n",
       "      <td>-47.792175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-46.896175</td>\n",
       "      <td>-40.126060</td>\n",
       "      <td>116.891319</td>\n",
       "      <td>-351.345337</td>\n",
       "      <td>86.865761</td>\n",
       "      <td>-99.941383</td>\n",
       "      <td>-94.355743</td>\n",
       "      <td>-35.766766</td>\n",
       "      <td>-103.297546</td>\n",
       "      <td>-206.596237</td>\n",
       "      <td>...</td>\n",
       "      <td>174.219254</td>\n",
       "      <td>207.147949</td>\n",
       "      <td>-160.191910</td>\n",
       "      <td>-72.079605</td>\n",
       "      <td>231.860718</td>\n",
       "      <td>160.584747</td>\n",
       "      <td>51.183266</td>\n",
       "      <td>3.251709</td>\n",
       "      <td>-20.809055</td>\n",
       "      <td>-19.630239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-24.852211</td>\n",
       "      <td>144.911758</td>\n",
       "      <td>-16.660511</td>\n",
       "      <td>143.682907</td>\n",
       "      <td>-99.156891</td>\n",
       "      <td>-38.815899</td>\n",
       "      <td>-70.038078</td>\n",
       "      <td>-80.872765</td>\n",
       "      <td>-86.241013</td>\n",
       "      <td>5.924940</td>\n",
       "      <td>...</td>\n",
       "      <td>265.224274</td>\n",
       "      <td>98.895889</td>\n",
       "      <td>248.775223</td>\n",
       "      <td>395.515076</td>\n",
       "      <td>-30.951685</td>\n",
       "      <td>-219.192673</td>\n",
       "      <td>-90.772003</td>\n",
       "      <td>110.593315</td>\n",
       "      <td>-107.129738</td>\n",
       "      <td>-313.334290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.900526</td>\n",
       "      <td>0.714963</td>\n",
       "      <td>-1.704514</td>\n",
       "      <td>1.345464</td>\n",
       "      <td>-1.145782</td>\n",
       "      <td>-2.345304</td>\n",
       "      <td>-0.432718</td>\n",
       "      <td>0.968551</td>\n",
       "      <td>1.278602</td>\n",
       "      <td>-2.060853</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.869221</td>\n",
       "      <td>1.397865</td>\n",
       "      <td>0.839844</td>\n",
       "      <td>-1.860224</td>\n",
       "      <td>2.509959</td>\n",
       "      <td>-1.883211</td>\n",
       "      <td>2.112453</td>\n",
       "      <td>1.338032</td>\n",
       "      <td>-2.202100</td>\n",
       "      <td>0.280150</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 128 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0           1           2           3          4           5    \\\n",
       "0   -4.028883   -2.877670    5.043583   -0.282113   9.059772    3.420488   \n",
       "1 -243.156174  -22.406778    8.421268  150.557510 -96.713753  138.841431   \n",
       "2  -46.896175  -40.126060  116.891319 -351.345337  86.865761  -99.941383   \n",
       "3  -24.852211  144.911758  -16.660511  143.682907 -99.156891  -38.815899   \n",
       "4   -1.900526    0.714963   -1.704514    1.345464  -1.145782   -2.345304   \n",
       "\n",
       "         6          7           8           9    ...         118         119  \\\n",
       "0  -2.221864  -0.936862    4.680723    0.022812  ...    1.390288   -2.516412   \n",
       "1 -76.178436 -37.150448   77.478683  102.910446  ...  106.663269   32.469856   \n",
       "2 -94.355743 -35.766766 -103.297546 -206.596237  ...  174.219254  207.147949   \n",
       "3 -70.038078 -80.872765  -86.241013    5.924940  ...  265.224274   98.895889   \n",
       "4  -0.432718   0.968551    1.278602   -2.060853  ...   -1.869221    1.397865   \n",
       "\n",
       "          120         121         122         123         124         125  \\\n",
       "0    6.060315   -0.654452   -0.875411   -1.368012    0.509950   -1.803684   \n",
       "1   66.770645   84.003662  107.856476  -17.861546 -122.073204 -141.799301   \n",
       "2 -160.191910  -72.079605  231.860718  160.584747   51.183266    3.251709   \n",
       "3  248.775223  395.515076  -30.951685 -219.192673  -90.772003  110.593315   \n",
       "4    0.839844   -1.860224    2.509959   -1.883211    2.112453    1.338032   \n",
       "\n",
       "          126         127  \n",
       "0    4.541414    0.619863  \n",
       "1    8.232184  -47.792175  \n",
       "2  -20.809055  -19.630239  \n",
       "3 -107.129738 -313.334290  \n",
       "4   -2.202100    0.280150  \n",
       "\n",
       "[5 rows x 128 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show result\n",
    "embedding_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c0b8cf35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame with 196591 rows and 128 columns\n"
     ]
    }
   ],
   "source": [
    "print(f'DataFrame with {embedding_df.shape[0]} rows and {embedding_df.shape[1]} columns')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "941ffcea",
   "metadata": {},
   "source": [
    "The next step is to create a single feature vector for each *pair* of users in our sample set. This vector needs to describe the relationship between the two users. The **Hadamard Product** is a simple yet effective method that should capture the interaction and agreement between two users embedding along each dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f3a84828",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Efficient vectorized Hadamard Product\n",
    "samples_df = pd.DataFrame(combined_samples, columns=['source', 'destination'])\n",
    "source_vectors = embedding_df.loc[(samples_df['source'])].values\n",
    "destination_vectors = embedding_df.loc[samples_df['destination']].values\n",
    "hadamard_product = source_vectors * destination_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1a12fe4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Pair Features dataframe with multi-index\n",
    "pair_features_df = pd.DataFrame(\n",
    "    hadamard_product,\n",
    "    index=pd.MultiIndex.from_frame(samples_df),\n",
    "    columns=[f'embed_hadamard_{i}' for i in range(hadamard_product.shape[1])]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d587d1a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>embed_hadamard_0</th>\n",
       "      <th>embed_hadamard_1</th>\n",
       "      <th>embed_hadamard_2</th>\n",
       "      <th>embed_hadamard_3</th>\n",
       "      <th>embed_hadamard_4</th>\n",
       "      <th>embed_hadamard_5</th>\n",
       "      <th>embed_hadamard_6</th>\n",
       "      <th>embed_hadamard_7</th>\n",
       "      <th>embed_hadamard_8</th>\n",
       "      <th>embed_hadamard_9</th>\n",
       "      <th>...</th>\n",
       "      <th>embed_hadamard_118</th>\n",
       "      <th>embed_hadamard_119</th>\n",
       "      <th>embed_hadamard_120</th>\n",
       "      <th>embed_hadamard_121</th>\n",
       "      <th>embed_hadamard_122</th>\n",
       "      <th>embed_hadamard_123</th>\n",
       "      <th>embed_hadamard_124</th>\n",
       "      <th>embed_hadamard_125</th>\n",
       "      <th>embed_hadamard_126</th>\n",
       "      <th>embed_hadamard_127</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>source</th>\n",
       "      <th>destination</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">0</th>\n",
       "      <th>1</th>\n",
       "      <td>979.647766</td>\n",
       "      <td>64.479317</td>\n",
       "      <td>42.473366</td>\n",
       "      <td>-42.474213</td>\n",
       "      <td>-876.204529</td>\n",
       "      <td>474.905457</td>\n",
       "      <td>169.258118</td>\n",
       "      <td>34.804832</td>\n",
       "      <td>362.656219</td>\n",
       "      <td>2.347621</td>\n",
       "      <td>...</td>\n",
       "      <td>148.292709</td>\n",
       "      <td>-81.707527</td>\n",
       "      <td>404.651123</td>\n",
       "      <td>-54.976391</td>\n",
       "      <td>-94.418739</td>\n",
       "      <td>24.434811</td>\n",
       "      <td>-62.251236</td>\n",
       "      <td>255.761078</td>\n",
       "      <td>37.385757</td>\n",
       "      <td>-29.624620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>188.939209</td>\n",
       "      <td>115.469574</td>\n",
       "      <td>589.551086</td>\n",
       "      <td>99.119041</td>\n",
       "      <td>786.983948</td>\n",
       "      <td>-341.848328</td>\n",
       "      <td>209.645630</td>\n",
       "      <td>33.508514</td>\n",
       "      <td>-483.507172</td>\n",
       "      <td>-4.712930</td>\n",
       "      <td>...</td>\n",
       "      <td>242.215027</td>\n",
       "      <td>-521.269531</td>\n",
       "      <td>-970.813354</td>\n",
       "      <td>47.172665</td>\n",
       "      <td>-202.973404</td>\n",
       "      <td>-219.681870</td>\n",
       "      <td>26.100908</td>\n",
       "      <td>-5.865055</td>\n",
       "      <td>-94.502533</td>\n",
       "      <td>-12.168067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100.126648</td>\n",
       "      <td>-417.008270</td>\n",
       "      <td>-84.028679</td>\n",
       "      <td>-40.534798</td>\n",
       "      <td>-898.338806</td>\n",
       "      <td>-132.769318</td>\n",
       "      <td>155.615082</td>\n",
       "      <td>75.766602</td>\n",
       "      <td>-403.670258</td>\n",
       "      <td>0.135161</td>\n",
       "      <td>...</td>\n",
       "      <td>368.738251</td>\n",
       "      <td>-248.862778</td>\n",
       "      <td>1507.656128</td>\n",
       "      <td>-258.845764</td>\n",
       "      <td>27.095444</td>\n",
       "      <td>299.858215</td>\n",
       "      <td>-46.289188</td>\n",
       "      <td>-199.475357</td>\n",
       "      <td>-486.520477</td>\n",
       "      <td>-194.224457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.656997</td>\n",
       "      <td>-2.057429</td>\n",
       "      <td>-8.596861</td>\n",
       "      <td>-0.379573</td>\n",
       "      <td>-10.380519</td>\n",
       "      <td>-8.022085</td>\n",
       "      <td>0.961441</td>\n",
       "      <td>-0.907399</td>\n",
       "      <td>5.984779</td>\n",
       "      <td>-0.047013</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.598756</td>\n",
       "      <td>-3.517605</td>\n",
       "      <td>5.089716</td>\n",
       "      <td>1.217428</td>\n",
       "      <td>-2.197246</td>\n",
       "      <td>2.576255</td>\n",
       "      <td>1.077245</td>\n",
       "      <td>-2.413386</td>\n",
       "      <td>-10.000648</td>\n",
       "      <td>0.173655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-177.432129</td>\n",
       "      <td>-129.863907</td>\n",
       "      <td>180.872009</td>\n",
       "      <td>1.508496</td>\n",
       "      <td>-1050.496216</td>\n",
       "      <td>-81.615990</td>\n",
       "      <td>-256.680695</td>\n",
       "      <td>16.716686</td>\n",
       "      <td>27.005644</td>\n",
       "      <td>-3.903668</td>\n",
       "      <td>...</td>\n",
       "      <td>-168.063080</td>\n",
       "      <td>-548.348328</td>\n",
       "      <td>-143.000809</td>\n",
       "      <td>-14.381987</td>\n",
       "      <td>-111.514908</td>\n",
       "      <td>151.453049</td>\n",
       "      <td>94.496552</td>\n",
       "      <td>-454.180786</td>\n",
       "      <td>-446.752899</td>\n",
       "      <td>50.445763</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 128 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    embed_hadamard_0  embed_hadamard_1  embed_hadamard_2  \\\n",
       "source destination                                                         \n",
       "0      1                  979.647766         64.479317         42.473366   \n",
       "       2                  188.939209        115.469574        589.551086   \n",
       "       3                  100.126648       -417.008270        -84.028679   \n",
       "       4                    7.656997         -2.057429         -8.596861   \n",
       "       5                 -177.432129       -129.863907        180.872009   \n",
       "\n",
       "                    embed_hadamard_3  embed_hadamard_4  embed_hadamard_5  \\\n",
       "source destination                                                         \n",
       "0      1                  -42.474213       -876.204529        474.905457   \n",
       "       2                   99.119041        786.983948       -341.848328   \n",
       "       3                  -40.534798       -898.338806       -132.769318   \n",
       "       4                   -0.379573        -10.380519         -8.022085   \n",
       "       5                    1.508496      -1050.496216        -81.615990   \n",
       "\n",
       "                    embed_hadamard_6  embed_hadamard_7  embed_hadamard_8  \\\n",
       "source destination                                                         \n",
       "0      1                  169.258118         34.804832        362.656219   \n",
       "       2                  209.645630         33.508514       -483.507172   \n",
       "       3                  155.615082         75.766602       -403.670258   \n",
       "       4                    0.961441         -0.907399          5.984779   \n",
       "       5                 -256.680695         16.716686         27.005644   \n",
       "\n",
       "                    embed_hadamard_9  ...  embed_hadamard_118  \\\n",
       "source destination                    ...                       \n",
       "0      1                    2.347621  ...          148.292709   \n",
       "       2                   -4.712930  ...          242.215027   \n",
       "       3                    0.135161  ...          368.738251   \n",
       "       4                   -0.047013  ...           -2.598756   \n",
       "       5                   -3.903668  ...         -168.063080   \n",
       "\n",
       "                    embed_hadamard_119  embed_hadamard_120  \\\n",
       "source destination                                           \n",
       "0      1                    -81.707527          404.651123   \n",
       "       2                   -521.269531         -970.813354   \n",
       "       3                   -248.862778         1507.656128   \n",
       "       4                     -3.517605            5.089716   \n",
       "       5                   -548.348328         -143.000809   \n",
       "\n",
       "                    embed_hadamard_121  embed_hadamard_122  \\\n",
       "source destination                                           \n",
       "0      1                    -54.976391          -94.418739   \n",
       "       2                     47.172665         -202.973404   \n",
       "       3                   -258.845764           27.095444   \n",
       "       4                      1.217428           -2.197246   \n",
       "       5                    -14.381987         -111.514908   \n",
       "\n",
       "                    embed_hadamard_123  embed_hadamard_124  \\\n",
       "source destination                                           \n",
       "0      1                     24.434811          -62.251236   \n",
       "       2                   -219.681870           26.100908   \n",
       "       3                    299.858215          -46.289188   \n",
       "       4                      2.576255            1.077245   \n",
       "       5                    151.453049           94.496552   \n",
       "\n",
       "                    embed_hadamard_125  embed_hadamard_126  embed_hadamard_127  \n",
       "source destination                                                              \n",
       "0      1                    255.761078           37.385757          -29.624620  \n",
       "       2                     -5.865055          -94.502533          -12.168067  \n",
       "       3                   -199.475357         -486.520477         -194.224457  \n",
       "       4                     -2.413386          -10.000648            0.173655  \n",
       "       5                   -454.180786         -446.752899           50.445763  \n",
       "\n",
       "[5 rows x 128 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show result\n",
    "pair_features_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ee198983",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free-up memory\n",
    "del embedding_df\n",
    "del samples_df\n",
    "del source_vectors\n",
    "del destination_vectors\n",
    "del hadamard_product"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a14c4a",
   "metadata": {},
   "source": [
    "### Domain Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e929b3c",
   "metadata": {},
   "source": [
    "Here is a palette of candidate features that were considered for the link prediction model:\n",
    "- **Radius of Gyration**: A measure of a user's typical travel radius, distinguishing \"stayers\" (small radius) from \"travelers\" (large radius).\n",
    "- **Total Check-in Count**: The total number of check-ins for a user, indicating their overall activity level.\n",
    "- **Unique Locations Count**: The number of distinct locations a user has visited, indicating their tendency to explore.\n",
    "- **Jaccard Similarity of Visited Locations**: The overlap in the set of unique locations visited by two users, measuring shared lifestyle and interests.\n",
    "- **Co-check-in Count**: The number of times two users checked into the same location within a short time window (e.g., 1 hour), indicating possible real-world interaction.\n",
    "- **Haversine Distance between User Centroids**: The geographic distance between the average location (centroid) of all check-ins for each user.\n",
    "- **Haversine Distance between Inferred Home Locations**: The geographic distance between the inferred \"home\" location of each user.\n",
    "- **Explicit Topological Features**: Common Neighbors, Jaccard Coefficient, Adamic-Adar, Resource Allocation, Preferential Attachment\n",
    "\n",
    "The first selection will be based on covering each domain of interaction with one feature that is thought to be the best:\n",
    "- **Radius of Gyration**: Analyzed in the EDA, represent the travelers vs. stayers concept and is an indicator of heterophily.\n",
    "- **Haversine Distance between Inferred Home Location**: the core finding in the EDA, represent the geographic proximity and has already been proved a strong indicator of homophily.\n",
    "- **Jaccard Similarity of Visited Locations**: we didn't explicitly explore this in the EDA, but could strongly represent the shared habits for a pair of users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c8e19ce6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Original Dataset] - Number of users with at least one check-in: 107092 (54.47%)\n",
      "[Time Train Dataset] - Number of users with at least one check-in: 63526 (32.31%)\n",
      "[Volume Train Dataset] - Number of users with at least one check-in: 94019 (47.82%)\n"
     ]
    }
   ],
   "source": [
    "checkins_per_user = checkins_df['user'].value_counts()\n",
    "print(f'[Original Dataset] - Number of users with at least one check-in: {len(checkins_per_user)} ({len(checkins_per_user)/G.number_of_nodes()*100:.2f}%)')\n",
    "checkins_per_user = t_train_df['user'].value_counts()\n",
    "print(f'[Time Train Dataset] - Number of users with at least one check-in: {len(checkins_per_user)} ({len(checkins_per_user)/G.number_of_nodes()*100:.2f}%)')\n",
    "checkins_per_user = v_train_df['user'].value_counts()\n",
    "print(f'[Volume Train Dataset] - Number of users with at least one check-in: {len(checkins_per_user)} ({len(checkins_per_user)/G.number_of_nodes()*100:.2f}%)')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea95ef45",
   "metadata": {},
   "source": [
    "#### Pair-Wise Radius of Gyration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "994ea39d",
   "metadata": {},
   "source": [
    "We need to recalculate the Radius of Gyration using only the check-ins before the cutoff date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "90cdc505",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Radius of Gyration with only check-ins before cutoff\n",
    "# Calculate Mean Centroid\n",
    "users_mean_centroids = t_train_df.groupby('user').agg(\n",
    "    mean_centroid_latitude=('latitude', 'mean'),\n",
    "    mean_centroid_longitude=('longitude', 'mean')\n",
    ")\n",
    "\n",
    "# Merge Mean Centroid with check-ins dataframe\n",
    "checkins_with_centroids = pd.merge(t_train_df, users_mean_centroids, on='user')\n",
    "checkins_with_centroids = checkins_with_centroids[['user', 'latitude', 'longitude', 'mean_centroid_latitude', 'mean_centroid_longitude']]\n",
    "\n",
    "# Zip to obtain coordinates pairs\n",
    "checkins_coords = list(zip(checkins_with_centroids['latitude'], checkins_with_centroids['longitude']))\n",
    "centroid_coords = list(zip(checkins_with_centroids['mean_centroid_latitude'], checkins_with_centroids['mean_centroid_longitude']))\n",
    "\n",
    "# Efficient vectorized Haversine Distance\n",
    "distances = haversine_vector(checkins_coords, centroid_coords)\n",
    "checkins_with_centroids['sq_distance_from_centroid'] = distances**2\n",
    "\n",
    "# Calculate Radius of Gyration and save in a new dataframe\n",
    "radius_of_gyration = checkins_with_centroids.groupby('user')['sq_distance_from_centroid'].mean().apply(np.sqrt).rename('radius_of_gyration_km')\n",
    "radius_of_gyration.index = radius_of_gyration.index.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b56a6e28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "user\n",
       "0     1203.823612\n",
       "4      421.415229\n",
       "5       49.801932\n",
       "9        5.595743\n",
       "10       8.475587\n",
       "Name: radius_of_gyration_km, dtype: float64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show results\n",
    "radius_of_gyration.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d3772a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the pair-wise feature: Radius of Gyration Absolute Difference\n",
    "samples_df = pd.DataFrame(combined_samples, columns=['source', 'destination'])\n",
    "source_rg_vector = samples_df['source'].map(radius_of_gyration)\n",
    "destination_rg_vector = samples_df['destination'].map(radius_of_gyration)\n",
    "abs_diff_rg = (source_rg_vector - destination_rg_vector).abs()\n",
    "\n",
    "rg_feature_df = pd.DataFrame({\n",
    "    'rg_source': source_rg_vector,\n",
    "    'rg_destination': destination_rg_vector,\n",
    "    'rg_abs_diff': abs_diff_rg\n",
    "})\n",
    "rg_feature_df.index = pd.MultiIndex.from_frame(samples_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "23d5e5e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rg_source          991583\n",
      "rg_destination    1110422\n",
      "rg_abs_diff       1470012\n",
      "dtype: int64 \n",
      "\n",
      "Radius of Gyration absolute difference missing for 1470012 (77.34%) pairs.\n"
     ]
    }
   ],
   "source": [
    "# Check for NaN values\n",
    "print(rg_feature_df.isna().sum(), \"\\n\")\n",
    "\n",
    "rg_na = rg_feature_df.isna().sum().loc['rg_abs_diff']\n",
    "print(f\"Radius of Gyration absolute difference missing for {rg_na} ({rg_na/len(combined_samples)*100:.4}%) pairs.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6001e20b",
   "metadata": {},
   "source": [
    "The **Radius of Gyration (RoG)** was calculated for all users based on their activity before the time-based cutoff date. As expected, and as a direct consequence of the network's growth, only 32.3% of the total users had check-in data within this period.\n",
    "\n",
    "We now need to deal with the presence of numerous missing values due to having many users not active on the platform in this time range. To treat this information as a predictive signal rather than a data problem, we engineered a dedicated categorical feature, `activity_status`, to explicitly describe the activity state of each pair (`Both Active`, `Source Active Only`, etc.). This allows the model to learn distinct patterns for each scenario. With a feature that gives context the the RoG values, all the NaN can now be filled with zeros. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "63468c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill the NaN values\n",
    "rg_feature_df['rg_source'] = rg_feature_df['rg_source'].fillna(0)\n",
    "rg_feature_df['rg_destination'] = rg_feature_df['rg_destination'].fillna(0)\n",
    "\n",
    "rg_feature_df['rg_abs_diff'] = (rg_feature_df['rg_source'] - rg_feature_df['rg_destination']).abs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f6e71624",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>rg_source</th>\n",
       "      <th>rg_destination</th>\n",
       "      <th>rg_abs_diff</th>\n",
       "      <th>activity_status</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>source</th>\n",
       "      <th>destination</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">0</th>\n",
       "      <th>1</th>\n",
       "      <td>1203.823612</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1203.823612</td>\n",
       "      <td>source_only</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1203.823612</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1203.823612</td>\n",
       "      <td>source_only</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1203.823612</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1203.823612</td>\n",
       "      <td>source_only</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1203.823612</td>\n",
       "      <td>421.415229</td>\n",
       "      <td>782.408383</td>\n",
       "      <td>both_have</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1203.823612</td>\n",
       "      <td>49.801932</td>\n",
       "      <td>1154.021681</td>\n",
       "      <td>both_have</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      rg_source  rg_destination  rg_abs_diff activity_status\n",
       "source destination                                                          \n",
       "0      1            1203.823612        0.000000  1203.823612     source_only\n",
       "       2            1203.823612        0.000000  1203.823612     source_only\n",
       "       3            1203.823612        0.000000  1203.823612     source_only\n",
       "       4            1203.823612      421.415229   782.408383       both_have\n",
       "       5            1203.823612       49.801932  1154.021681       both_have"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add categorical feature for context\n",
    "source_is_active = samples_df['source'].isin(radius_of_gyration.index)\n",
    "destination_is_active = samples_df['destination'].isin(radius_of_gyration.index)\n",
    "\n",
    "conditions = [\n",
    "    source_is_active & destination_is_active,\n",
    "    source_is_active & ~destination_is_active,\n",
    "    ~source_is_active & destination_is_active,\n",
    "    ~source_is_active & ~destination_is_active\n",
    "]\n",
    "choices = ['both_have', 'source_only', 'dest_only', 'neither_have']\n",
    "\n",
    "activity_status = np.select(conditions, choices, default='Error')\n",
    "\n",
    "rg_feature_df['activity_status'] = activity_status\n",
    "rg_feature_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "233f4dfb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>rg_source</th>\n",
       "      <th>rg_destination</th>\n",
       "      <th>rg_abs_diff</th>\n",
       "      <th>rg_status_both_have</th>\n",
       "      <th>rg_status_dest_only</th>\n",
       "      <th>rg_status_neither_have</th>\n",
       "      <th>rg_status_source_only</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>source</th>\n",
       "      <th>destination</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">0</th>\n",
       "      <th>1</th>\n",
       "      <td>1203.823612</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1203.823612</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1203.823612</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1203.823612</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1203.823612</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1203.823612</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1203.823612</td>\n",
       "      <td>421.415229</td>\n",
       "      <td>782.408383</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1203.823612</td>\n",
       "      <td>49.801932</td>\n",
       "      <td>1154.021681</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      rg_source  rg_destination  rg_abs_diff  \\\n",
       "source destination                                             \n",
       "0      1            1203.823612        0.000000  1203.823612   \n",
       "       2            1203.823612        0.000000  1203.823612   \n",
       "       3            1203.823612        0.000000  1203.823612   \n",
       "       4            1203.823612      421.415229   782.408383   \n",
       "       5            1203.823612       49.801932  1154.021681   \n",
       "\n",
       "                    rg_status_both_have  rg_status_dest_only  \\\n",
       "source destination                                             \n",
       "0      1                          False                False   \n",
       "       2                          False                False   \n",
       "       3                          False                False   \n",
       "       4                           True                False   \n",
       "       5                           True                False   \n",
       "\n",
       "                    rg_status_neither_have  rg_status_source_only  \n",
       "source destination                                                 \n",
       "0      1                             False                   True  \n",
       "       2                             False                   True  \n",
       "       3                             False                   True  \n",
       "       4                             False                  False  \n",
       "       5                             False                  False  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# One-hot encoding categorical feature\n",
    "rg_feature_df = pd.get_dummies(rg_feature_df, columns=['activity_status'], prefix='rg_status')\n",
    "rg_feature_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fd8b435d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add results to the complete pair feature dataframe\n",
    "pair_features_df = pair_features_df.join(rg_feature_df.drop(columns=['rg_source', 'rg_destination']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0d66db16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free-Up Memory\n",
    "del users_mean_centroids\n",
    "del checkins_with_centroids\n",
    "del checkins_coords\n",
    "del centroid_coords\n",
    "del distances\n",
    "del radius_of_gyration\n",
    "del samples_df\n",
    "del source_rg_vector\n",
    "del destination_rg_vector\n",
    "del abs_diff_rg\n",
    "del rg_feature_df\n",
    "del source_is_active\n",
    "del destination_is_active\n",
    "del activity_status"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf83b3a",
   "metadata": {},
   "source": [
    "#### Haversine Distance between Inferred Home Locations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed5c1a54",
   "metadata": {},
   "source": [
    "Again, all the calculations needs to be redone using only the check-ins before the cutoff date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f6aa385",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reuse code from EDA\n",
    "inferred_home_df = t_train_df.groupby('user').agg(\n",
    "    median_centroid_latitude=('latitude', 'median'),\n",
    "    median_centroid_longitude=('longitude', 'median')\n",
    ")\n",
    "\n",
    "# Heuristic to infer Home Locations\n",
    "home_hours = (t_train_df['check-in_datetime'].dt.hour >= 21) | (t_train_df['check-in_datetime'].dt.hour < 7)\n",
    "\n",
    "# The grid will have 25x25 Kilometers cells\n",
    "lat_step, lon_step = 0.25, 0.25\n",
    "t_train_df['lat_bin'] = (t_train_df['latitude'] / lat_step).astype(int)\n",
    "t_train_df['lon_bin'] = (t_train_df['longitude'] / lon_step).astype(int)\n",
    "\n",
    "# Find the most visited grid cell for each user during home hours\n",
    "home_cells = t_train_df[home_hours]\\\n",
    "    .groupby(['user', 'lat_bin', 'lon_bin'])\\\n",
    "    .size()\\\n",
    "    .reset_index(name='count')\\\n",
    "    .sort_values('count', ascending=False)\\\n",
    "    .drop_duplicates(subset='user')\\\n",
    "    .set_index('user')\\\n",
    "    [['lat_bin', 'lon_bin']]\\\n",
    "    .rename(columns={'lat_bin': 'home_lat_bin', 'lon_bin': 'home_lon_bin'})\n",
    "\n",
    "# Chain operations to calculate the centroid of check-ins within each user's home cell\n",
    "home_cell_centroids = (\n",
    "    t_train_df.join(home_cells, on='user')\n",
    "    .dropna(subset=['home_lat_bin'])\n",
    "    .query('lat_bin == home_lat_bin and lon_bin == home_lon_bin')\n",
    "    .groupby('user')\n",
    "    .agg(\n",
    "        home_cell_centroid_lat=('latitude', 'median'),\n",
    "        home_cell_centroid_lon=('longitude', 'median')\n",
    "    )\n",
    ")\n",
    "\n",
    "inferred_home_df = inferred_home_df.join(home_cell_centroids)\n",
    "inferred_home_df.index = inferred_home_df.index.astype(int)\n",
    "inferred_home_df = inferred_home_df[['home_cell_centroid_lat', 'home_cell_centroid_lon']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0045af80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "home_cell_centroid_lat    10739\n",
       "home_cell_centroid_lon    10739\n",
       "dtype: int64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inferred_home_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "955c9788",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_df = pd.DataFrame(combined_samples, columns=[\"source\", \"destination\"])\n",
    "\n",
    "samples_df = samples_df.join(inferred_home_df, on=\"source\")\n",
    "samples_df = samples_df.rename(columns={\n",
    "    \"home_cell_centroid_lat\": \"source_home_lat\",\n",
    "    \"home_cell_centroid_lon\": \"source_home_lon\",\n",
    "})\n",
    "\n",
    "samples_df = samples_df.join(inferred_home_df, on=\"destination\")\n",
    "samples_df = samples_df.rename(columns={\n",
    "    \"home_cell_centroid_lat\": \"dest_home_lat\",\n",
    "    \"home_cell_centroid_lon\": \"dest_home_lon\",\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "10fd6da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_df.index = pd.MultiIndex.from_frame(samples_df[[\"source\", \"destination\"]])\n",
    "samples_df = samples_df.drop(columns=[\"source\", \"destination\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "50949ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_home_coords = list(zip(samples_df['source_home_lat'], samples_df['source_home_lon']))\n",
    "dest_home_coords = list(zip(samples_df['dest_home_lat'], samples_df['dest_home_lon']))\n",
    "\n",
    "distances = haversine_vector(source_home_coords, dest_home_coords)\n",
    "samples_df['home_distance_km'] = distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f1abb90b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source_home_lat     1060573\n",
      "source_home_lon     1060573\n",
      "dest_home_lat       1201301\n",
      "dest_home_lon       1201301\n",
      "home_distance_km    1539456\n",
      "dtype: int64 \n",
      "\n",
      "Home Distance missing for 1539456 (81.0%) pairs.\n"
     ]
    }
   ],
   "source": [
    "# Check for NaN values\n",
    "print(samples_df.isna().sum(), \"\\n\")\n",
    "\n",
    "home_dist_na = samples_df.isna().sum().loc['home_distance_km']\n",
    "print(f\"Home Distance missing for {home_dist_na} ({home_dist_na/len(combined_samples)*100:.4}%) pairs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "387a59b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_has_home = samples_df['source_home_lat'].notna()\n",
    "dest_has_home = samples_df['dest_home_lat'].notna()\n",
    "\n",
    "conditions = [\n",
    "    source_has_home & dest_has_home,\n",
    "    source_has_home & ~dest_has_home,\n",
    "    ~source_has_home & dest_has_home,\n",
    "    ~source_has_home & ~dest_has_home\n",
    "]\n",
    "\n",
    "choices = ['both_have', 'source_only', 'dest_only', 'neither_have']\n",
    "samples_df['home_status'] = np.select(conditions, choices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0374771a",
   "metadata": {},
   "source": [
    "The `home_distance_km` column now contains the calculated distances for pairs where both users had an inferred home, and NaN for all other pairs. A simple imputation with zero would be misleading, as it would imply that users with no inferred home live at the same location. Therefore, a more neutral and statistically robust approach is chosen: the missing distance values are imputed using the **median** of all the calculated distances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "22942e07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Median Home Distance:  858.9177190455824\n"
     ]
    }
   ],
   "source": [
    "# Impute the missing home distance values with the median of the distances\n",
    "median_distance = samples_df['home_distance_km'].median()\n",
    "samples_df['home_distance_km'] = samples_df['home_distance_km'].fillna(median_distance)\n",
    "\n",
    "print(\"Median Home Distance: \", median_distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b16df100",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encoding categorical feature\n",
    "samples_df = pd.get_dummies(samples_df, columns=['home_status'], prefix='home_status')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2d41fc17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add results to the complete pair feature dataframe\n",
    "pair_features_df = pair_features_df.join(samples_df.drop(columns=['source_home_lat', 'source_home_lon', 'dest_home_lat', 'dest_home_lon']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "db694820",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free-Up Memory\n",
    "del inferred_home_df\n",
    "del home_cells\n",
    "del home_hours\n",
    "del home_cell_centroids\n",
    "del samples_df\n",
    "del source_home_coords\n",
    "del dest_home_coords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778e302a",
   "metadata": {},
   "source": [
    "#### Jaccard Similarity of Visited Locations"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn_env_py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
